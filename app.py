import os
import json
import re
import requests
from flask import Flask, request, jsonify
from dotenv import load_dotenv

# ============== OpenAI (t√πy ch·ªçn, ƒë·ªÉ hi·ªÉu intent & m∆∞·ª£t c√¢u tr·∫£ l·ªùi) ==============
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# ============== ENV ==============
load_dotenv()

TELEGRAM_TOKEN        = os.getenv("TELEGRAM_TOKEN", "")
HOTLINE_TUYEN_TREN    = os.getenv("HOTLINE_TUYEN_TREN", "09xx.xxx.xxx")
LINK_KENH_TELEGRAM    = os.getenv("LINK_KENH_TELEGRAM", "https://t.me/...")
LINK_FANPAGE          = os.getenv("LINK_FANPAGE", "https://facebook.com/...")
LINK_WEBSITE          = os.getenv("LINK_WEBSITE", "https://...")
OPENAI_API_KEY        = os.getenv("OPENAI_API_KEY", "")
LOG_SHEET_WEBHOOK_URL = os.getenv("LOG_SHEET_WEBHOOK_URL", "")

# Chat id nh√≥m / leader tuy·∫øn tr√™n ƒë·ªÉ forward y√™u c·∫ßu h·ªó tr·ª£
UPLINE_CHAT_ID        = os.getenv("UPLINE_CHAT_ID", "")  # v√≠ d·ª•: "-1001234567890"

ENABLE_AI_POLISH      = os.getenv("ENABLE_AI_POLISH", "true").lower() == "true"

# L∆∞u tr·∫°ng th√°i: TVV v·ª´a b·∫•m "K·∫øt n·ªëi tuy·∫øn tr√™n" v√† ƒëang chu·∫©n b·ªã g·ª≠i c√¢u h·ªèi
ESCALATION_PENDING: dict[int, bool] = {}  # {chat_id: True}

if not TELEGRAM_TOKEN:
    raise RuntimeError("Ch∆∞a c·∫•u h√¨nh TELEGRAM_TOKEN trong .env")

TELEGRAM_API = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}"

# ============== OpenAI client (n·∫øu c√≥) ==============
client = None
if OPENAI_API_KEY and OpenAI is not None:
    client = OpenAI(api_key=OPENAI_API_KEY)

# ============== Load data (products.json + combos.json) ==============
BASE_DIR = os.path.dirname(__file__)
DATA_DIR = os.path.join(BASE_DIR, "data")

with open(os.path.join(DATA_DIR, "products.json"), "r", encoding="utf-8") as f:
    PRODUCTS_DATA = json.load(f)
with open(os.path.join(DATA_DIR, "combos.json"), "r", encoding="utf-8") as f:
    COMBOS_DATA = json.load(f)

# Ch·∫•p nh·∫≠n format {"products":[...]} ho·∫∑c list th·∫≥ng
PRODUCTS = PRODUCTS_DATA.get("products", PRODUCTS_DATA)
COMBOS   = COMBOS_DATA.get("combos", COMBOS_DATA)

# Load th√™m metadata health_tags + tri·ªáu ch·ª©ng + alias b·ªï sung (n·∫øu c√≥)
try:
    with open(os.path.join(DATA_DIR, "health_tags_info.json"), "r", encoding="utf-8") as f:
        HEALTH_TAGS_INFO = json.load(f)
except FileNotFoundError:
    HEALTH_TAGS_INFO = {}

try:
    with open(os.path.join(DATA_DIR, "symptoms_map.json"), "r", encoding="utf-8") as f:
        SYMPTOMS_MAP_RAW = json.load(f)
except FileNotFoundError:
    SYMPTOMS_MAP_RAW = {}

try:
    with open(os.path.join(DATA_DIR, "product_aliases.json"), "r", encoding="utf-8") as f:
        PRODUCT_ALIASES_DATA = json.load(f)
        PRODUCT_ALIASES_BY_ALIAS = PRODUCT_ALIASES_DATA.get("by_alias", PRODUCT_ALIASES_DATA)
except FileNotFoundError:
    PRODUCT_ALIASES_BY_ALIAS = {}

HEALTH_TAG_LABELS = {
    "tieu_duong": "h·ªó tr·ª£ ·ªïn ƒë·ªãnh ƒë∆∞·ªùng huy·∫øt, ti·ªÉu ƒë∆∞·ªùng",
    "tieu_hoa": "h·ªó tr·ª£ ti√™u h√≥a, ƒë∆∞·ªùng ru·ªôt",
    "gan": "h·ªó tr·ª£ ch·ª©c nƒÉng gan, th·∫£i ƒë·ªôc gan",
    "thai_doc": "th·∫£i ƒë·ªôc, gi·∫£i ƒë·ªôc c∆° th·ªÉ",
    "mien_dich": "tƒÉng c∆∞·ªùng h·ªá mi·ªÖn d·ªãch",
    "tim_mach": "h·ªó tr·ª£ tim m·∫°ch, huy·∫øt √°p",
    "xuong_khop": "h·ªó tr·ª£ x∆∞∆°ng kh·ªõp, gi·∫£m ƒëau kh·ªõp",
    "than": "h·ªó tr·ª£ th·∫≠n ‚Äì ti·∫øt ni·ªáu",
    "ung_thu": "h·ªó tr·ª£ b·ªánh l√Ω/u b∆∞·ªõu, ung th∆∞ (k·∫øt h·ª£p ph√°c ƒë·ªì)",
    "giam_mo": "gi·∫£m m·ª°, ki·ªÉm so√°t c√¢n n·∫∑ng",
}
# B·ªï sung/ghi ƒë√® nh√£n t·ª´ file health_tags_info.json (n·∫øu c√≥)
if HEALTH_TAGS_INFO:
    for _tag, _info in HEALTH_TAGS_INFO.items():
        _lbl = (_info.get("label") or "").strip()
        if _lbl:
            HEALTH_TAG_LABELS[_tag] = _lbl

def build_usecase_from_tags(tags):
    labels = []
    for t in tags or []:
        lbl = HEALTH_TAG_LABELS.get(t)
        if lbl and lbl not in labels:
            labels.append(lbl)
    return "; ".join(labels)

# ---------- Helper: ki·ªÉm tra h·∫øt h√†ng ----------
def is_product_out_of_stock(p: dict) -> bool:
    """
    Quy ∆∞·ªõc hi·ªán t·∫°i:
    - N·∫øu c√≥ field in_stock = False ‚Üí h·∫øt h√†ng.
    - N·∫øu kh√¥ng c√≥ link (product_url/url) ‚Üí coi nh∆∞ t·∫°m h·∫øt h√†ng.
    """
    if isinstance(p.get("in_stock"), bool):
        return not p["in_stock"]
    url = (p.get("product_url") or p.get("url") or "").strip()
    return url == ""

# ---------- Helper chu·∫©n h√≥a & health tags ----------

def normalize_for_match(s: str) -> str:
    """Lower + b·ªè d·∫•u + b·ªè k√Ω t·ª± l·∫° ƒë·ªÉ so kh·ªõp alias/keyword."""
    import unicodedata
    if not s:
        return ""
    s = str(s).lower().strip()
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# Map keyword ‚Üí health_tag (kh√¥ng ph·ª• thu·ªôc d·ªØ li·ªáu, anh c√≥ th·ªÉ b·ªï sung d·∫ßn)
_HEALTH_KEYWORD_TO_TAG_RAW = {
    "ti·ªÉu ƒë∆∞·ªùng": "tieu_duong",
    "dai thao duong": "tieu_duong",
    "ƒë√°i th√°o ƒë∆∞·ªùng": "tieu_duong",
    "duong huyet": "tieu_duong",
    "ƒë∆∞·ªùng huy·∫øt": "tieu_duong",

    "da day": "da_day",
    "d·∫° d√†y": "da_day",
    "bao tu": "da_day",
    "bao t·ª≠": "da_day",
    "trao nguoc": "da_day",
    "tr√†o ng∆∞·ª£c": "da_day",
    "o chua": "da_day",
    "·ª£ chua": "da_day",

    "tieu hoa": "tieu_hoa",
    "ti√™u h√≥a": "tieu_hoa",
    "tieu ho√°": "tieu_hoa",
    "tao bon": "tieu_hoa",
    "t√°o b√≥n": "tieu_hoa",

    "gan": "gan",
    "men gan": "gan",
    "gan nhiem mo": "gan",
    "gan nhi·ªÖm m·ª°": "gan",

    "xuong khop": "xuong_khop",
    "x∆∞∆°ng kh·ªõp": "xuong_khop",
    "dau khop": "xuong_khop",
    "ƒëau kh·ªõp": "xuong_khop",
    "gout": "xuong_khop",

    "huyet ap": "tim_mach",
    "huy·∫øt √°p": "tim_mach",
    "tim mach": "tim_mach",
    "tim m·∫°ch": "tim_mach",

    "thai doc": "thai_doc",
    "th·∫£i ƒë·ªôc": "thai_doc",
    "detox": "thai_doc",

    "ung thu": "ung_thu",
    "ung th∆∞": "ung_thu",
}

HEALTH_KEYWORD_TO_TAG = {
    normalize_for_match(k): v for k, v in _HEALTH_KEYWORD_TO_TAG_RAW.items()
}

# Chu·∫©n h√≥a symptoms_map t·ª´ file JSON: key (tri·ªáu ch·ª©ng) ‚Üí list health_tags
SYMPTOMS_MAP_NORM = {}
if isinstance(SYMPTOMS_MAP_RAW, dict):
    for raw_symptom, info in SYMPTOMS_MAP_RAW.items():
        tags = []
        if isinstance(info, dict):
            tags = info.get("health_tags", []) or []
        elif isinstance(info, list):
            tags = info
        key_norm = normalize_for_match(raw_symptom)
        if key_norm and tags:
            SYMPTOMS_MAP_NORM[key_norm] = tags

def extract_health_tags_from_text(text: str):
    """Tr√≠ch health_tags t·ª´ c√¢u m√¥ t·∫£ tri·ªáu ch·ª©ng/b·ªánh l√Ω."""
    nt = normalize_for_match(text)
    tags: set[str] = set()

    # 1) Theo tri·ªáu ch·ª©ng trong file JSON
    for sym_norm, tags_list in SYMPTOMS_MAP_NORM.items():
        if sym_norm and sym_norm in nt:
            for t in tags_list:
                if t:
                    tags.add(t)

    # 2) Theo keyword map c·ª©ng (b·ªï sung)
    for kw_norm, tag in HEALTH_KEYWORD_TO_TAG.items():
        if kw_norm and kw_norm in nt:
            tags.add(tag)

    return tags


def build_product_aliases(p: dict):
    """Sinh th√™m alias t·ª´ name + code + aliases g·ªëc."""
    aliases = set()
    name = p.get("name", "")
    code = str(p.get("code", "")).lstrip("#").strip()
    if code:
        p["code"] = code

    if name:
        aliases.add(name)
        aliases.add(name.lower())
        for part in re.findall(r"[\w\u00C0-\u017F\-\/]+", name):
            aliases.add(part)

    for a in p.get("aliases", []):
        if a:
            aliases.add(a)

    if code:
        aliases.add(code)

    aliases_clean = []
    for a in aliases:
        a2 = re.sub(r"\s+", " ", str(a)).strip()
        if a2:
            aliases_clean.append(a2)

    p["aliases"] = aliases_clean

def build_combo_aliases(c: dict):
    aliases = set()
    name = c.get("name", "")
    if name:
        aliases.add(name)
        aliases.add(name.lower())
        for part in re.findall(r"[\w\u00C0-\u017F\-\/]+", name):
            aliases.add(part)
    for a in c.get("aliases", []):
        if a:
            aliases.add(a)
    aliases_clean = []
    for a in aliases:
        a2 = re.sub(r"\s+", " ", str(a)).strip()
        if a2:
            aliases_clean.append(a2)
    c["aliases"] = aliases_clean

# ---------- Build PRODUCTS + alias index + health_tags ----------

PRODUCT_MAP: dict[str, dict] = {}
PRODUCT_ALIAS_INDEX: dict[str, set[str]] = {}   # alias_norm ‚Üí set(code)

for p in PRODUCTS:
    build_product_aliases(p)
    code = p.get("code")
    if not code:
        continue

    # G·∫Øn health_tags (k·∫øt h·ª£p tag c√≥ s·∫µn trong JSON + detect t·ª´ text)
    current_tags = set(p.get("health_tags", []))
    text_for_tags = " ".join([
        p.get("name", ""),
        p.get("benefits_text", "") or p.get("benefits", "") or "",
        p.get("ingredients_text", "") or p.get("ingredients", "") or "",
        p.get("usage_text", "") or p.get("usage", "") or "",
    ])
    auto_tags = extract_health_tags_from_text(text_for_tags)
    all_tags = sorted(current_tags.union(auto_tags))
    if all_tags:
        p["health_tags"] = all_tags

    PRODUCT_MAP[code] = p

    for a in p["aliases"]:
        na = normalize_for_match(a)
        if not na:
            continue
        PRODUCT_ALIAS_INDEX.setdefault(na, set()).add(code)

# B·ªï sung alias t·ª´ file product_aliases.json (n·∫øu c√≥)
for alias_norm, codes in PRODUCT_ALIASES_BY_ALIAS.items():
    na = normalize_for_match(alias_norm)
    if not na:
        continue
    for code in codes:
        if not code:
            continue
        PRODUCT_ALIAS_INDEX.setdefault(na, set()).add(str(code))

# ---------- Build COMBOS + alias index + health_tags ----------

COMBO_ID_MAP: dict[str, dict] = {}
COMBO_ALIAS_INDEX: dict[str, list[dict]] = {}   # alias_norm ‚Üí [combo]

for c in COMBOS:
    build_combo_aliases(c)
    cid = c.get("id") or normalize_for_match(c.get("name", "") or "")
    c["id"] = cid

    combo_tags = set(c.get("health_tags", []))
    text_for_tags = " ".join([
        c.get("name", ""),
        c.get("header_text", ""),
        c.get("duration_text", ""),
    ])
    combo_tags |= extract_health_tags_from_text(text_for_tags)

    for item in c.get("products", []):
        code = str(item.get("product_code", "")).lstrip("#").strip()
        item["product_code"] = code
        p = PRODUCT_MAP.get(code)
        if p:
            item.setdefault("name", p.get("name", ""))
            item.setdefault("price_text", p.get("price_text", ""))
            item.setdefault("product_url", p.get("product_url", ""))
            for t in p.get("health_tags", []):
                combo_tags.add(t)

    if combo_tags:
        c["health_tags"] = sorted(combo_tags)

    COMBO_ID_MAP[cid] = c

    for a in c["aliases"]:
        na = normalize_for_match(a)
        if not na:
            continue
        COMBO_ALIAS_INDEX.setdefault(na, []).append(c)

# ============== Telegram Keyboard ==============
MAIN_KEYBOARD = {
    "keyboard": [
        [
            {"text": "üß© Combo theo v·∫•n ƒë·ªÅ s·ª©c kh·ªèe"},
            {"text": "üîé Tra c·ª©u s·∫£n ph·∫©m"}
        ],
        [
            {"text": "üõí H∆∞·ªõng d·∫´n mua h√†ng"},
            {"text": "‚òéÔ∏è K·∫øt n·ªëi tuy·∫øn tr√™n"}
        ],
        [
            {"text": "üì¢ K√™nh & Fanpage"}
        ]
    ],
    "resize_keyboard": True,
    "one_time_keyboard": False
}

# ============== Flask app ==============
app = Flask(__name__)

# ============== Helper: g·ª≠i message Telegram ==============
def send_message(chat_id, text, reply_markup=None, parse_mode="Markdown"):
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": parse_mode
    }
    if reply_markup is not None:
        payload["reply_markup"] = json.dumps(reply_markup, ensure_ascii=False)

    url = f"{TELEGRAM_API}/sendMessage"
    try:
        requests.post(url, data=payload, timeout=10)
    except Exception as e:
        print("Error sending message:", e)

# ============== Helper: utility ==============
def contains_any(text, keywords):
    text = text.lower()
    return any(k.lower() in text for k in keywords)

def extract_code(text: str):
    """B·∫Øt m√£ s·∫£n ph·∫©m d·∫°ng 0xxxxx."""
    text = text.strip()
    codes = re.findall(r"\b0\d{4,5}\b", text)
    return codes[0] if codes else None

def find_best_products(text: str, limit: int = 5):
    """T√¨m s·∫£n ph·∫©m theo alias (name, m√£, alias m·ªü r·ªông)."""
    t = normalize_for_match(text)
    results = []
    seen = set()

    for alias_norm, codes in PRODUCT_ALIAS_INDEX.items():
        if alias_norm and alias_norm in t:
            for code in codes:
                if code not in seen and code in PRODUCT_MAP:
                    seen.add(code)
                    results.append(PRODUCT_MAP[code])
                    if len(results) >= limit:
                        return results

    if not results:
        tokens = t.split()
        for alias_norm, codes in PRODUCT_ALIAS_INDEX.items():
            if any(tok in alias_norm for tok in tokens):
                for code in codes:
                    if code not in seen and code in PRODUCT_MAP:
                        seen.add(code)
                        results.append(PRODUCT_MAP[code])
                        if len(results) >= limit:
                            return results

    return results

def find_products_by_health(text: str, limit: int = 5):
    """T√¨m s·∫£n ph·∫©m theo health_tags (t·ª´ JSON) + t·ª´ kh√≥a trong c√¢u."""
    tags_from_text = extract_health_tags_from_text(text)
    results = []
    seen = set()

    if tags_from_text:
        for p in PRODUCTS:
            p_tags = set(p.get("health_tags", []))
            if p_tags.intersection(tags_from_text):
                code = p.get("code")
                if code and code not in seen:
                    seen.add(code)
                    results.append(p)
                    if len(results) >= limit:
                        break

    if not results:
        results = find_best_products(text, limit=limit)

    return results

def find_best_combo(text: str, limit: int = 3):
    t = normalize_for_match(text)
    results = []
    seen = set()

    for alias_norm, combos in COMBO_ALIAS_INDEX.items():
        if alias_norm and alias_norm in t:
            for c in combos:
                cid = c.get("id")
                if cid not in seen:
                    seen.add(cid)
                    results.append(c)
                    if len(results) >= limit:
                        return results
    return results

def find_combo_by_health_keyword(text: str) -> dict | None:
    tags_from_text = extract_health_tags_from_text(text)
    best = None
    score_best = 0
    text_norm = normalize_for_match(text)

    for c in COMBOS:
        c_tags = set(c.get("health_tags", []))
        score = len(c_tags.intersection(tags_from_text)) if tags_from_text else 0
        for a in c.get("aliases", []):
            if normalize_for_match(a) in text_norm:
                score += 1
        if score > score_best:
            score_best = score
            best = c

    if not best:
        combos = find_best_combo(text, limit=1)
        best = combos[0] if combos else None

    return best

# ============== Orchestrator: ph√¢n t√≠ch c√¢u h·ªèi & g·ª£i √Ω combo/s·∫£n ph·∫©m ==============

def parse_user_query_with_ai(text: str) -> dict:
    base = {
        "symptoms": [],
        "goals": [],
        "need_meal_plan": False,
        "target": "auto",
        "raw_text": text,
    }
    if not client:
        return base

    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            temperature=0,
            response_format={"type": "json_object"},
            messages=[
                {
                    "role": "system",
                    "content": (
                        "B·∫°n l√† b·ªô ph√¢n t√≠ch c√¢u h·ªèi cho chatbot h·ªó tr·ª£ t∆∞ v·∫•n vi√™n th·ª±c ph·∫©m ch·ª©c nƒÉng.\n"
                        "Tr·∫£ v·ªÅ JSON:\n"
                        "{\n"
                        '  \"symptoms\": [...],\n'
                        '  \"goals\": [...],\n'
                        '  \"need_meal_plan\": true/false,\n'
                        '  \"target\": \"combo\" | \"product\" | \"info\" | \"auto\"\n'
                        "}\n"
                        "Ch·ªâ tr·∫£ JSON, kh√¥ng gi·∫£i th√≠ch th√™m."
                    ),
                },
                {"role": "user", "content": text},
            ],
        )
        content = resp.choices[0].message.content or ""
        data = json.loads(content)
    except Exception as e:
        print("parse_user_query_with_ai error:", e)
        return base

    parsed = dict(base)
    syms = data.get("symptoms")
    if isinstance(syms, list):
        parsed["symptoms"] = [str(s).strip() for s in syms if s]
    goals = data.get("goals")
    if isinstance(goals, list):
        parsed["goals"] = [str(g).strip() for g in goals if g]
    parsed["need_meal_plan"] = bool(data.get("need_meal_plan", False))
    target = str(data.get("target", "auto") or "auto").lower()
    if target not in ("combo", "product", "info", "auto"):
        target = "auto"
    parsed["target"] = target

    return parsed


def rank_combos_and_products(parsed: dict, limit_combos: int = 3, limit_products: int = 5) -> dict:
    text = parsed.get("raw_text") or ""
    text_norm = normalize_for_match(text)
    tags: set[str] = set()

    for s in parsed.get("symptoms") or []:
        tags.update(extract_health_tags_from_text(s))
    for g in parsed.get("goals") or []:
        tags.update(extract_health_tags_from_text(g))

    if not tags:
        tags.update(extract_health_tags_from_text(text))

    combo_scores: list[tuple[float, dict]] = []
    for c in COMBOS:
        c_tags = set(c.get("health_tags", []))
        if not c_tags:
            continue
        score = 0.0
        inter = c_tags.intersection(tags)
        if inter:
            score += 2.0 * len(inter)

        for a in c.get("aliases", []):
            na = normalize_for_match(a)
            if na and na in text_norm:
                score += 1.0
                break

        if score > 0:
            combo_scores.append((score, c))

    combo_scores.sort(key=lambda x: x[0], reverse=True)
    top_combos = [c for score, c in combo_scores[:limit_combos]]

    product_scores: list[tuple[float, dict]] = []
    for p in PRODUCTS:
        p_tags = set(p.get("health_tags", []))
        if not p_tags:
            continue
        score = 0.0

        inter = p_tags.intersection(tags)
        if inter:
            score += 2.0 * len(inter)

        for a in p.get("aliases", []):
            na = normalize_for_match(a)
            if na and na in text_norm:
                score += 1.0
                break

        code = str(p.get("code") or "").strip()
        if code and code in text_norm.replace(" ", ""):
            score += 3.0

        # Gi·∫£m ∆∞u ti√™n s·∫£n ph·∫©m h·∫øt h√†ng
        if is_product_out_of_stock(p):
            score -= 1.0

        if score > 0:
            product_scores.append((score, p))

    product_scores.sort(key=lambda x: x[0], reverse=True)
    top_products = [p for score, p in product_scores[:limit_products]]

    return {
        "tags": list(tags),
        "combos": top_combos,
        "products": top_products,
    }


def build_meal_plan_snippet(parsed: dict) -> str:
    if not parsed.get("need_meal_plan"):
        return ""
    lines = []
    lines.append("\nüçΩ *G·ª£i √Ω khung b·ªØa ƒÉn ƒëi k√®m:*")
    lines.append("- S√°ng: Y·∫øn m·∫°ch + tr·ª©ng/·ª©c g√† + 1 ph·∫ßn tr√°i c√¢y (t√°o/cam).")
    lines.append("- Tr∆∞a: ·ª®c g√†/c√° + khoai lang/g·∫°o l·ª©t + nhi·ªÅu rau xanh.")
    lines.append("- T·ªëi: C√°/ƒë·∫≠u ph·ª• + rau c·ªß + n·∫•m, h·∫°n ch·∫ø tinh b·ªôt nhanh.")
    lines.append("- U·ªëng 1.5‚Äì2L n∆∞·ªõc/ng√†y, h·∫°n ch·∫ø n∆∞·ªõc ng·ªçt c√≥ ƒë∆∞·ªùng, r∆∞·ª£u bia.")
    lines.append("- N·∫øu t·∫≠p luy·ªán: b·ªØa ph·ª• tr∆∞·ªõc/sau t·∫≠p (chu·ªëi + s·ªØa chua kh√¥ng ƒë∆∞·ªùng).")
    return "\n".join(lines)


def orchestrate_health_answer(text: str, intent: str):
    """
    Tr·∫£ v·ªÅ: reply_text, matched_combo, matched_product, parsed, ranking
    """
    parsed = parse_user_query_with_ai(text)
    ranking = rank_combos_and_products(parsed)
    combos = ranking.get("combos") or []
    products = ranking.get("products") or []

    reply = ""
    matched_combo = None
    matched_product = None

    if intent == "combo_health":
        if combos:
            matched_combo = combos[0]
            reply = format_combo_answer(matched_combo)
        elif products:
            matched_product = products[0]
            reply = format_products_answer(products)
        else:
            combo_old = find_combo_by_health_keyword(text)
            if combo_old:
                matched_combo = combo_old
                reply = format_combo_answer(combo_old)
            else:
                products_old = find_products_by_health(text)
                if products_old:
                    matched_product = products_old[0]
                reply = format_products_answer(products_old)

    elif intent == "health_products":
        if products:
            matched_product = products[0]
            reply = format_products_answer(products)
        elif combos:
            matched_combo = combos[0]
            reply = format_combo_answer(matched_combo)
        else:
            products_old = find_products_by_health(text)
            if products_old:
                matched_product = products_old[0]
            reply = format_products_answer(products_old)

    elif intent == "product_info":
        if products:
            matched_product = products[0]
            reply = format_products_answer(products)
        else:
            products_old = find_best_products(text)
            if products_old:
                matched_product = products_old[0]
            reply = format_products_answer(products_old)

    meal_plan = build_meal_plan_snippet(parsed)
    if meal_plan:
        reply = f"{reply}{meal_plan}"

    return reply, matched_combo, matched_product, parsed, ranking

# ============== AI: ph√¢n lo·∫°i intent ==============
INTENT_LABELS = [
    "start",
    "buy_payment",
    "business_escalation",
    "business_escalation_detail",
    "channels",
    "combo_health",
    "product_info",
    "product_by_code",
    "health_products",
    "menu_combo",
    "menu_product_search",
    "menu_buy_payment",
    "menu_business_escalation",
    "menu_channels",
    "fallback"
]

def classify_intent_ai(text: str):
    if not client:
        return None
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            temperature=0,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an intent classifier for a Telegram bot helping health product advisors.\n"
                        "Return ONLY ONE of these labels:\n"
                        f"{', '.join(INTENT_LABELS)}\n\n"
                        "Meaning:\n"
                        "- start: greeting or /start\n"
                        "- buy_payment: how to buy/pay/order\n"
                        "- business_escalation: hard business/commission/policy questions\n"
                        "- business_escalation_detail: follow-up message describing the hard question for upline\n"
                        "- channels: official channels, fanpage, website\n"
                        "- combo_health: which combo for a health problem\n"
                        "- product_info: ask about a product by name or description\n"
                        "- product_by_code: ask using a product code (e.g. 070728)\n"
                        "- health_products: ask for products for a health issue (not necessarily a combo)\n"
                        "- menu_* : when pressing menu buttons with those meanings\n"
                        "- fallback: anything else\n"
                        "Answer with ONLY the label, no explanation."
                    )
                },
                {"role": "user", "content": text}
            ]
        )
        label = resp.choices[0].message.content.strip().lower()
        if label in INTENT_LABELS:
            return label
    except Exception as e:
        print("Error classify_intent_ai:", e)
    return None

def classify_intent_rules(text: str):
    t = text.lower().strip()

    if "combo theo v·∫•n ƒë·ªÅ" in t:
        return "menu_combo"
    if "tra c·ª©u s·∫£n ph·∫©m" in t:
        return "menu_product_search"
    if "h∆∞·ªõng d·∫´n mua h√†ng" in t:
        return "menu_buy_payment"
    if "k·∫øt n·ªëi tuy·∫øn tr√™n" in t:
        return "menu_business_escalation"
    if "k√™nh & fanpage" in t or "k√™nh & fan" in t or "k√™nh v√† fanpage" in t:
        return "menu_channels"

    if t.startswith("/start") or "b·∫Øt ƒë·∫ßu" in t or "hello" in t:
        return "start"

    code = extract_code(t)
    if code and code in PRODUCT_MAP:
        return "product_by_code"

    if contains_any(t, ["mua h√†ng", "ƒë·∫∑t h√†ng", "ƒë·∫∑t mua", "thanh to√°n", "tr·∫£ ti·ªÅn", "ship", "giao h√†ng"]):
        return "buy_payment"

    if contains_any(t, ["tuy·∫øn tr√™n", "leader", "sponsor", "upline", "kh√≥ tr·∫£ l·ªùi", "h·ªèi gi√∫p"]):
        return "business_escalation"

    if contains_any(t, ["k√™nh", "kenh", "fanpage", "facebook", "page", "k√™nh ch√≠nh th·ª©c"]):
        return "channels"

    if contains_any(t, [
        "ti·ªÉu ƒë∆∞·ªùng", "ƒë√°i th√°o ƒë∆∞·ªùng", "ƒë∆∞·ªùng huy·∫øt",
        "d·∫° d√†y", "bao t·ª≠", "tr√†o ng∆∞·ª£c", "·ª£ chua",
        "c∆° x∆∞∆°ng kh·ªõp", "ƒëau kh·ªõp", "gout",
        "huy·∫øt √°p", "tim m·∫°ch",
        "gan", "men gan", "gan nhi·ªÖm m·ª°",
        "ti√™u h√≥a", "r·ªëi lo·∫°n ti√™u h√≥a", "t√°o b√≥n"
    ]):
        return "combo_health"

    if contains_any(t, ["th√†nh ph·∫ßn", "t√°c d·ª•ng", "l·ª£i √≠ch", "c√°ch d√πng", "c√¥ng d·ª•ng", "u·ªëng nh∆∞ th·∫ø n√†o"]):
        return "product_info"

    if find_best_combo(t):
        return "combo_health"
    if find_best_products(t):
        return "product_info"

    return "fallback"

def classify_intent(text: str):
    label = classify_intent_ai(text)
    if label:
        return label
    return classify_intent_rules(text)

# ============== AI: m∆∞·ª£t h√≥a c√¢u tr·∫£ l·ªùi ==============
def polish_answer_with_ai(answer: str) -> str:
    if not client or not ENABLE_AI_POLISH:
        return answer
    try:
        sys_prompt = (
            "B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi cho ƒë·ªôi t∆∞ v·∫•n vi√™n s·∫£n ph·∫©m s·ª©c kh·ªèe.\n"
            "H√£y vi·∫øt l·∫°i c√¢u tr·∫£ l·ªùi ti·∫øng Vi·ªát cho t·ª± nhi√™n, r√µ r√†ng, d·ªÖ copy g·ª≠i cho kh√°ch.\n"
            "Y√äU C·∫¶U B·∫ÆT BU·ªòC:\n"
            "- KH√îNG th√™m b·∫•t k·ª≥ claim/l·ª£i √≠ch/th√¥ng tin m·ªõi n√†o ngo√†i n·ªôi dung ƒë√£ c√≥.\n"
            "- GI·ªÆ NGUY√äN t·∫•t c·∫£ t√™n s·∫£n ph·∫©m, m√£ s·∫£n ph·∫©m, gi√°, ƒë∆∞·ªùng link URL, li·ªÅu d√πng.\n"
            "- N·∫øu c√≥ c·∫£nh b√°o/l∆∞u √Ω trong n·ªôi dung g·ªëc, ph·∫£i gi·ªØ nguy√™n.\n"
        )
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            temperature=0.4,
            messages=[
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": answer}
            ]
        )
        new_answer = resp.choices[0].message.content.strip()
        return new_answer or answer
    except Exception as e:
        print("Error polish_answer_with_ai:", e)
        return answer

# ============== Format tr·∫£ l·ªùi ==============
def format_combo_answer(combo):
    name     = combo.get("name", "Combo")
    header   = combo.get("header_text", "")
    duration = combo.get("duration_text", "")

    lines = [f"*{name}*"]
    if header:
        lines.append(f"_{header}_")
    if duration:
        lines.append(f"\n‚è± *Th·ªùi gian khuy·∫øn ngh·ªã:* {duration}")

    combo_usecase = build_usecase_from_tags(combo.get("health_tags", []))
    if combo_usecase:
        lines.append(f"\nüéØ *Combo n√†y ph√π h·ª£p:* {combo_usecase}")

    lines.append("\nüß© *C√°c s·∫£n ph·∫©m trong combo:*")

    products_info = []
    for item in combo.get("products", []):
        code = (item.get("product_code") or "").strip()
        dose = (item.get("dose_text") or "").strip()

        p = PRODUCT_MAP.get(code, {}) if code else {}

        pname       = item.get("name")        or p.get("name")        or code
        price       = item.get("price_text")  or p.get("price_text", "")
        url         = item.get("product_url") or p.get("product_url", "")
        benefits    = item.get("benefits_text")    or p.get("benefits_text")    or p.get("benefits", "")
        ingredients = item.get("ingredients_text") or p.get("ingredients_text") or p.get("ingredients", "")
        usage       = item.get("usage_text")       or p.get("usage_text")       or p.get("usage", "")
        tags        = item.get("health_tags")      or p.get("health_tags", [])
        usecase     = build_usecase_from_tags(tags)

        block = f"‚Ä¢ *{pname}* ({code})"
        if price:
            block += f"\n  - Gi√° tham kh·∫£o: {price}"
        if benefits:
            block += f"\n  - L·ª£i √≠ch ch√≠nh: {benefits}"
        if usecase:
            block += f"\n  - D√πng trong c√°c tr∆∞·ªùng h·ª£p: {usecase}"
        if ingredients:
            block += f"\n  - Th√†nh ph·∫ßn n·ªïi b·∫≠t: {ingredients}"

        if usage and dose and usage.strip() != dose.strip():
            block += f"\n  - C√°ch d√πng theo NSX: {usage}"
            block += f"\n  - C√°ch d√πng g·ª£i √Ω trong combo: {dose}"
        elif dose:
            block += f"\n  - C√°ch d√πng g·ª£i √Ω: {dose}"
        elif usage:
            block += f"\n  - C√°ch d√πng g·ª£i √Ω: {usage}"

        # H·∫øt h√†ng / c√≤n h√†ng
        if is_product_out_of_stock(p):
            block += "\n  - ‚ö†Ô∏è S·∫£n ph·∫©m n√†y hi·ªán t·∫°m h·∫øt h√†ng tr√™n h·ªá th·ªëng, anh/ch·ªã vui l√≤ng li√™n h·ªá kho ho·∫∑c tham kh·∫£o s·∫£n ph·∫©m kh√°c ph√π h·ª£p."
        elif url:
            block += f"\n  - üîó Link s·∫£n ph·∫©m: {url}"

        products_info.append(block)

    lines.append("\n" + "\n\n".join(products_info))
    lines.append(
        "\n‚ö†Ô∏è L∆∞u √Ω: ƒê√¢y l√† combo h·ªó tr·ª£, kh√¥ng thay th·∫ø thu·ªëc ƒëi·ªÅu tr·ªã. "
        "TVV n√™n nh·∫Øc kh√°ch tu√¢n th·ªß t∆∞ v·∫•n c·ªßa b√°c sƒ©, k·∫øt h·ª£p ch·∫ø ƒë·ªô ƒÉn u·ªëng, v·∫≠n ƒë·ªông, t√°i kh√°m ƒë·ªãnh k·ª≥."
    )
    lines.append("\nüëâ TVV c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh c√¢u ch·ªØ cho ph√π h·ª£p v·ªõi kh√°ch h√†ng c·ª• th·ªÉ.")
    return "\n".join(lines)

def format_products_answer(products):
    if not products:
        return (
            "Em ch∆∞a t√¨m ƒë∆∞·ª£c s·∫£n ph·∫©m ph√π h·ª£p trong danh m·ª•c hi·ªán c√≥ ·∫°. üôè\n"
            "Anh/ch·ªã c√≥ th·ªÉ g·ª≠i r√µ h∆°n t√™n s·∫£n ph·∫©m, m√£ s·∫£n ph·∫©m ho·∫∑c v·∫•n ƒë·ªÅ s·ª©c kh·ªèe c·ªßa kh√°ch gi√∫p em."
        )

    lines = ["D∆∞·ªõi ƒë√¢y l√† *m·ªôt s·ªë s·∫£n ph·∫©m ph√π h·ª£p* trong danh m·ª•c:\n"]
    for p in products[:5]:
        name        = p.get("name", "")
        code        = p.get("code", "")
        ingredients = p.get("ingredients_text", "") or p.get("ingredients", "")
        usage       = p.get("usage_text", "")       or p.get("usage", "")
        benefits    = p.get("benefits_text", "")    or p.get("benefits", "")
        url         = p.get("product_url", "")      or p.get("url", "")
        price       = p.get("price_text", "")       or p.get("price", "")
        tags        = p.get("health_tags", [])
        usecase     = build_usecase_from_tags(tags)

        block = f"*{name}* ({code})"
        if price:
            block += f"\n- Gi√° tham kh·∫£o: {price}"
        if benefits:
            block += f"\n- L·ª£i √≠ch ch√≠nh: {benefits}"
        if usecase:
            block += f"\n- D√πng trong c√°c tr∆∞·ªùng h·ª£p: {usecase}"
        if ingredients:
            block += f"\n- Th√†nh ph·∫ßn n·ªïi b·∫≠t: {ingredients}"
        if usage:
            block += f"\n- C√°ch d√πng g·ª£i √Ω: {usage}"
        if is_product_out_of_stock(p):
            block += "\n- ‚ö†Ô∏è S·∫£n ph·∫©m n√†y hi·ªán t·∫°m h·∫øt h√†ng tr√™n h·ªá th·ªëng, anh/ch·ªã vui l√≤ng li√™n h·ªá kho ho·∫∑c tham kh·∫£o s·∫£n ph·∫©m kh√°c."
        elif url:
            block += f"\n- üîó Link s·∫£n ph·∫©m: {url}"
        lines.append(block)
        lines.append("")

    lines.append(
        "üëâ TVV h√£y ch·ªçn s·∫£n ph·∫©m ph√π h·ª£p nh·∫•t v·ªõi t√¨nh tr·∫°ng c·ª• th·ªÉ c·ªßa kh√°ch, "
        "v√† lu√¥n nh·∫Øc kh√°ch ƒë·ªçc k·ªπ h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng, tham kh·∫£o √Ω ki·∫øn b√°c sƒ© khi c·∫ßn."
    )
    return "\n".join(lines)

def format_product_by_code(code: str):
    p = PRODUCT_MAP.get(code)
    if not p:
        return "Em ch∆∞a t√¨m th·∫•y m√£ s·∫£n ph·∫©m n√†y ·∫°. Anh/ch·ªã ki·ªÉm tra l·∫°i gi√∫p em m√£ s·ªë nh√©. üôè"

    name        = p.get("name", "")
    ingredients = p.get("ingredients_text", "") or p.get("ingredients", "")
    usage       = p.get("usage_text", "")       or p.get("usage", "")
    benefits    = p.get("benefits_text", "")    or p.get("benefits", "")
    url         = p.get("product_url", "")      or p.get("url", "")
    price       = p.get("price_text", "")       or p.get("price", "")
    tags        = p.get("health_tags", [])
    usecase     = build_usecase_from_tags(tags)

    lines = [f"*{name}* ({code})"]
    if price:
        lines.append(f"- Gi√° tham kh·∫£o: {price}")
    if benefits:
        lines.append(f"- L·ª£i √≠ch ch√≠nh: {benefits}")
    if usecase:
        lines.append(f"- D√πng trong c√°c tr∆∞·ªùng h·ª£p: {usecase}")
    if ingredients:
        lines.append(f"- Th√†nh ph·∫ßn n·ªïi b·∫≠t: {ingredients}")
    if usage:
        lines.append(f"- C√°ch d√πng g·ª£i √Ω: {usage}")
    if is_product_out_of_stock(p):
        lines.append("- ‚ö†Ô∏è S·∫£n ph·∫©m n√†y hi·ªán t·∫°m h·∫øt h√†ng tr√™n h·ªá th·ªëng, anh/ch·ªã vui l√≤ng li√™n h·ªá kho ho·∫∑c tham kh·∫£o s·∫£n ph·∫©m kh√°c.")
    elif url:
        lines.append(f"- üîó Link s·∫£n ph·∫©m: {url}")
    lines.append(
        "\nüëâ TVV c√≥ th·ªÉ ch·ªânh s·ª≠a c√¢u ch·ªØ cho ph√π h·ª£p v·ªõi kh√°ch, "
        "v√† nh·∫Øc kh√°ch ƒë·ªçc k·ªπ h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng, tham kh·∫£o √Ω ki·∫øn b√°c sƒ© khi c·∫ßn."
    )
    return "\n".join(lines)

# ============== C√°c c√¢u menu / c·ªë ƒë·ªãnh ==============
def answer_start():
    return (
        "*Ch√†o TVV, em l√† Tr·ª£ l√Ω AI h·ªó tr·ª£ kinh doanh & s·∫£n ph·∫©m.* ü§ñ\n\n"
        "Anh/ch·ªã c√≥ th·ªÉ:\n"
        "‚Ä¢ H·ªèi theo v·∫•n ƒë·ªÅ s·ª©c kh·ªèe: _\"Kh√°ch b·ªã ti·ªÉu ƒë∆∞·ªùng th√¨ d√πng combo n√†o?\"_\n"
        "‚Ä¢ H·ªèi theo s·∫£n ph·∫©m: _\"Cho em th√†nh ph·∫ßn, c√°ch d√πng c·ªßa m√£ 070728\"_\n"
        "‚Ä¢ H·ªèi quy tr√¨nh: _\"H∆∞·ªõng d·∫´n mua h√†ng / thanh to√°n th·∫ø n√†o?\"_\n"
        "‚Ä¢ Nh·ªù tuy·∫øn tr√™n: _\"C√¢u n√†y kh√≥, cho em xin k·∫øt n·ªëi leader?\"_\n\n"
        "Ho·∫∑c b·∫•m c√°c n√∫t menu b√™n d∆∞·ªõi ƒë·ªÉ thao t√°c nhanh. ‚ù§Ô∏è"
    )

def answer_menu_combo():
    return (
        "üß© *Combo theo v·∫•n ƒë·ªÅ s·ª©c kh·ªèe*\n\n"
        "Anh/ch·ªã h√£y g√µ c√¢u d·∫°ng:\n"
        "- \"Kh√°ch *ti·ªÉu ƒë∆∞·ªùng* th√¨ d√πng combo n√†o?\"\n"
        "- \"Kh√°ch b·ªã *c∆° x∆∞∆°ng kh·ªõp* ƒëau nhi·ªÅu th√¨ t∆∞ v·∫•n combo g√¨?\"\n"
        "- \"Kh√°ch b·ªã *huy·∫øt √°p, tim m·∫°ch* th√¨ n√™n d√πng g√¨?\""
    )

def answer_menu_product_search():
    return (
        "üîé *Tra c·ª©u s·∫£n ph·∫©m*\n\n"
        "Anh/ch·ªã c√≥ th·ªÉ h·ªèi:\n"
        "- \"Cho em info s·∫£n ph·∫©m *ANTISWEET*?\"\n"
        "- \"Th√†nh ph·∫ßn, c√°ch d√πng c·ªßa m√£ *070728* l√† g√¨?\"\n"
        "- \"S·∫£n ph·∫©m n√†o h·ªó tr·ª£ *ti·ªÉu ƒë∆∞·ªùng / men gan / x∆∞∆°ng kh·ªõp*?\""
    )

def answer_buy_payment():
    lines = []
    lines.append("*H∆∞·ªõng d·∫´n mua h√†ng & thanh to√°n* üõí")
    lines.append("\n1Ô∏è‚É£ *C√°ch mua h√†ng:*")
    lines.append(f"- ƒê·∫∑t tr·ª±c ti·∫øp tr√™n website: {LINK_WEBSITE}")
    lines.append("- Nh·ªù TVV t·∫°o ƒë∆°n h√†ng tr√™n h·ªá th·ªëng.")
    lines.append("- G·ªçi Hotline ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ t·∫°o ƒë∆°n.")
    lines.append("\n2Ô∏è‚É£ *C√°c b∆∞·ªõc ƒë·∫∑t tr√™n website (g·ª£i √Ω):*")
    lines.append("   1. Truy c·∫≠p website.")
    lines.append("   2. Ch·ªçn s·∫£n ph·∫©m ‚Üí b·∫•m *‚ÄúTh√™m v√†o gi·ªè‚Äù*.")
    lines.append("   3. V√†o *Gi·ªè h√†ng* ‚Üí ki·ªÉm tra s·∫£n ph·∫©m.")
    lines.append("   4. B·∫•m *‚ÄúThanh to√°n‚Äù* ‚Üí nh·∫≠p th√¥ng tin nh·∫≠n h√†ng.")
    lines.append("   5. Ch·ªçn h√¨nh th·ª©c thanh to√°n ph√π h·ª£p.")
    lines.append("\n3Ô∏è‚É£ *H√¨nh th·ª©c thanh to√°n th∆∞·ªùng d√πng:*")
    lines.append("- üíµ Thanh to√°n khi nh·∫≠n h√†ng (COD).")
    lines.append("- üí≥ Chuy·ªÉn kho·∫£n ng√¢n h√†ng (theo s·ªë TK ch√≠nh th·ª©c c·ªßa c√¥ng ty).")
    lines.append("- üì± Thanh to√°n online (QR, v√≠ ƒëi·ªán t·ª≠‚Ä¶) n·∫øu c√≥.")
    return "\n".join(lines)

def answer_business_escalation():
    return (
        "*K·∫øt n·ªëi tuy·∫øn tr√™n khi g·∫∑p c√¢u h·ªèi kh√≥* ‚òéÔ∏è\n\n"
        "Anh/ch·ªã h√£y g·ª≠i ti·∫øp *1 tin nh·∫Øn n·ªØa* m√¥ t·∫£ r√µ:\n"
        "- C√¢u h·ªèi / t√¨nh hu·ªëng c·ª• th·ªÉ c·ªßa kh√°ch\n"
        "- Ph∆∞∆°ng √°n anh/ch·ªã ƒëang ph√¢n v√¢n ho·∫∑c ƒë√£ tr·∫£ l·ªùi th·ª≠\n"
        "- M·ª©c ƒë·ªô g·∫•p (vd: c·∫ßn h·ªó tr·ª£ trong h√¥m nay)\n\n"
        "Ngay sau tin nh·∫Øn ƒë√≥, em s·∫Ω *chuy·ªÉn nguy√™n vƒÉn* cho tuy·∫øn tr√™n ƒë·ªÉ h·ªó tr·ª£.\n"
        f"N·∫øu th·∫≠t s·ª± g·∫•p, anh/ch·ªã c√≥ th·ªÉ g·ªçi th√™m Hotline: *{HOTLINE_TUYEN_TREN}*."
    )

def answer_channels():
    return (
        "*K√™nh & Fanpage ch√≠nh th·ª©c c·ªßa c√¥ng ty* üì¢\n\n"
        f"- üì∫ K√™nh Telegram: {LINK_KENH_TELEGRAM}\n"
        f"- üëç Fanpage Facebook: {LINK_FANPAGE}\n"
        f"- üåê Website: {LINK_WEBSITE}\n\n"
        "üëâ TVV n√™n ∆∞u ti√™n g·ª≠i kh√°ch c√°c ƒë∆∞·ªùng link ch√≠nh th·ª©c n√†y."
    )

def answer_fallback():
    return (
        "Hi·ªán t·∫°i em ch∆∞a hi·ªÉu r√µ c√¢u h·ªèi ho·∫∑c ch∆∞a c√≥ d·ªØ li·ªáu cho n·ªôi dung n√†y ·∫°. üôè\n\n"
        "Anh/ch·ªã c√≥ th·ªÉ:\n"
        "- M√¥ t·∫£ *c·ª• th·ªÉ h∆°n* t√¨nh tr·∫°ng c·ªßa kh√°ch, ho·∫∑c\n"
        "- H·ªèi d·∫°ng: \"Kh√°ch b·ªã *ti·ªÉu ƒë∆∞·ªùng*...\", \"Kh√°ch b·ªã *ƒëau d·∫° d√†y*...\", "
        "\"*C√°ch mua h√†ng*?\", \"*Thanh to√°n th·∫ø n√†o*?\", ho·∫∑c\n"
        "- B·∫•m n√∫t *K·∫øt n·ªëi tuy·∫øn tr√™n* ƒë·ªÉ em h∆∞·ªõng d·∫´n li√™n h·ªá leader."
    )

# ============== Logging l√™n Google Sheets ==============
def log_to_sheet(payload: dict):
    if not LOG_SHEET_WEBHOOK_URL:
        return
    try:
        requests.post(LOG_SHEET_WEBHOOK_URL, json=payload, timeout=5)
    except Exception as e:
        print("Error log_to_sheet:", e)

# ============== Webhook ch√≠nh ==============
@app.route("/webhook", methods=["POST"])
def webhook():
    update = request.get_json(force=True)
    message = update.get("message") or update.get("edited_message")
    if not message:
        return jsonify(ok=True)

    chat_id   = message["chat"]["id"]
    text      = message.get("text", "") or ""
    from_user = message.get("from", {}) or {}
    user_name = (from_user.get("first_name", "") + " " +
                 from_user.get("last_name", "")).strip() or from_user.get("username", "")

    # ===== Tin nh·∫Øn t·ª´ nh√≥m tuy·∫øn tr√™n =====
    if UPLINE_CHAT_ID and str(chat_id) == str(UPLINE_CHAT_ID) and text.strip():
        target_chat_id = None
        reply_body = None

        m = re.match(r"^/reply\s+(-?\d+)\s+(.+)", text.strip(), re.DOTALL | re.IGNORECASE)
        if m:
            target_chat_id = int(m.group(1))
            reply_body = m.group(2).strip()
        else:
            reply_msg = message.get("reply_to_message") or {}
            base_text = reply_msg.get("text") or ""
            m2 = re.search(r"chat_id:\s*`(-?\d+)`", base_text)
            if m2:
                target_chat_id = int(m2.group(1))
                reply_body = text.strip()

        if target_chat_id and reply_body:
            tvv_reply = f"*Tr·∫£ l·ªùi t·ª´ tuy·∫øn tr√™n:* üëá\n\n{reply_body}"
            tvv_reply = polish_answer_with_ai(tvv_reply)
            send_message(target_chat_id, tvv_reply, reply_markup=MAIN_KEYBOARD)

            log_payload = {
                "chat_id": target_chat_id,
                "user_name": user_name,
                "text": reply_body,
                "intent": "upline_reply",
                "matched_combo_id": "",
                "matched_combo_name": "",
                "matched_product_code": "",
                "matched_product_name": "",
                "upline_name": user_name,
                "from_upline_chat_id": chat_id,
            }
            log_to_sheet(log_payload)

        return jsonify(ok=True)

    # ===== Tin nh·∫Øn t·ª´ TVV =====
    if not text:
        send_message(chat_id, "Hi·ªán t·∫°i em ch·ªâ hi·ªÉu tin nh·∫Øn d·∫°ng text th√¥i ·∫°. üôè", reply_markup=MAIN_KEYBOARD)
        return jsonify(ok=True)

    # N·∫øu ƒëang ch·ªù m√¥ t·∫£ cho tuy·∫øn tr√™n
    if ESCALATION_PENDING.get(chat_id):
        ESCALATION_PENDING.pop(chat_id, None)

        if UPLINE_CHAT_ID:
            notify = (
                "üîî *Y√äU C·∫¶U H·ªñ TR·ª¢ TUY·∫æN TR√äN*\n\n"
                f"- T·ª´ TVV: *{user_name}* (chat_id: `{chat_id}`)\n"
                f"- N·ªôi dung:\n{text}"
            )
            try:
                send_message(UPLINE_CHAT_ID, notify)
            except Exception as e:
                print("Error forward to upline:", e)

        confirm = (
            "Em ƒë√£ ghi nh·∫≠n v√† *chuy·ªÉn n·ªôi dung n√†y cho tuy·∫øn tr√™n* r·ªìi ·∫°. ‚úÖ\n"
            f"N·∫øu c·∫ßn g·∫•p, anh/ch·ªã c√≥ th·ªÉ g·ªçi th√™m Hotline: *{HOTLINE_TUYEN_TREN}*.\n"
            "Khi tuy·∫øn tr√™n ph·∫£n h·ªìi, anh/ch·ªã nh·ªõ c·∫≠p nh·∫≠t l·∫°i cho kh√°ch nh√©."
        )
        confirm = polish_answer_with_ai(confirm)
        send_message(chat_id, confirm, reply_markup=MAIN_KEYBOARD)

        log_payload = {
            "chat_id": chat_id,
            "user_name": user_name,
            "text": text,
            "intent": "business_escalation_detail",
            "matched_combo_id": "",
            "matched_combo_name": "",
            "matched_product_code": "",
            "matched_product_name": "",
        }
        log_to_sheet(log_payload)

        return jsonify(ok=True)

    # ===== B√¨nh th∆∞·ªùng: ph√¢n lo·∫°i intent =====
    intent = classify_intent(text)

    matched_combo_id      = ""
    matched_combo_name    = ""
    matched_product_code  = ""
    matched_product_name  = ""

    parsed_for_log  = None
    ranking_for_log = None

    # X·ª≠ l√Ω intent
    if intent == "start":
        reply = answer_start()

    elif intent == "menu_combo":
        reply = answer_menu_combo()

    elif intent == "menu_product_search":
        reply = answer_menu_product_search()

    elif intent in ("menu_buy_payment", "buy_payment"):
        reply = answer_buy_payment()

    elif intent in ("menu_business_escalation", "business_escalation"):
        ESCALATION_PENDING[chat_id] = True
        reply = answer_business_escalation()

    elif intent in ("menu_channels", "channels"):
        reply = answer_channels()

    elif intent == "product_by_code":
        code = extract_code(text)
        if code and code in PRODUCT_MAP:
            reply = format_product_by_code(code)
            matched_product_code = code
            matched_product_name = PRODUCT_MAP[code].get("name", "")
        else:
            reply = "Em ch∆∞a t√¨m ƒë∆∞·ª£c m√£ s·∫£n ph·∫©m n√†y, anh/ch·ªã ki·ªÉm tra l·∫°i gi√∫p em nh√©. üôè"

    elif intent in ("combo_health", "health_products", "product_info"):
        reply, combo, product, parsed_for_log, ranking_for_log = orchestrate_health_answer(text, intent)

        if combo:
            matched_combo_id   = combo.get("id", "")
            matched_combo_name = combo.get("name", "")
        if product:
            matched_product_code = product.get("code", "")
            matched_product_name = product.get("name", "")

    else:
        reply = answer_fallback()

        # M∆∞·ª£t h√≥a b·∫±ng OpenAI (n·∫øu b·∫≠t)
    reply = polish_answer_with_ai(reply)

    # G·ª≠i l·∫°i cho TVV
    send_message(chat_id, reply, reply_markup=MAIN_KEYBOARD)

    # ---------------- LOG PH·ª§C V·ª§ AUTO-LEARNING ----------------
    parsed_symptoms = parsed_for_log.get("symptoms") if parsed_for_log else []
    parsed_goals    = parsed_for_log.get("goals") if parsed_for_log else []
    parsed_target   = parsed_for_log.get("target") if parsed_for_log else ""
    need_meal_plan  = bool(parsed_for_log.get("need_meal_plan")) if parsed_for_log else False
    health_tags     = ranking_for_log.get("tags") if ranking_for_log else []

    # Chu·∫©n b·ªã top combos/products ƒë·ªÉ log (d√πng cho ph√¢n t√≠ch & auto-learning)
    ranked_combos_list   = ranking_for_log.get("combos")   if ranking_for_log else []
    ranked_products_list = ranking_for_log.get("products") if ranking_for_log else []

    ranked_combos = [
        {
            "id": c.get("id"),
            "name": c.get("name"),
            "health_tags": c.get("health_tags", []),
        }
        for c in ranked_combos_list
    ]

    ranked_products = [
        {
            "code": p.get("code"),
            "name": p.get("name"),
            "health_tags": p.get("health_tags", []),
        }
        for p in ranked_products_list
    ]

    log_payload = {
        "chat_id": chat_id,
        "user_name": user_name,
        "text": text,             # C√¢u TVV g·ª≠i
        "bot_reply": reply,       # C√¢u Bot tr·∫£ l·ªùi (ƒë·ªÉ ph√¢n t√≠ch c√°ch tr·∫£ l·ªùi)
        "intent": intent,

        "parsed_symptoms": parsed_symptoms,
        "parsed_goals": parsed_goals,
        "parsed_target": parsed_target,
        "need_meal_plan": need_meal_plan,
        "health_tags": health_tags,

        "matched_combo_id": matched_combo_id,
        "matched_combo_name": matched_combo_name,
        "matched_product_code": matched_product_code,
        "matched_product_name": matched_product_name,

        "ranked_combos": ranked_combos,
        "ranked_products": ranked_products,

        # Auto-learning V1: kh√¥ng d√πng final_* / feedback th·ªß c√¥ng n·ªØa,
        # nh∆∞ng v·∫´n ƒë·ªÉ s·∫µn n·∫øu sau n√†y m√¨nh mu·ªën d√πng.
        "final_combo_id": "",
        "final_product_code": "",
        "feedback": "",
    }
    log_to_sheet(log_payload)

    return jsonify(ok=True)

@app.route("/healthz", methods=["GET"])
def healthz():
    return "ok", 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000, debug=True)
