import os
import json
import re
import unicodedata
import requests
from flask import Flask, request, jsonify
from dotenv import load_dotenv

# ============== OpenAI (ƒë·ªÉ hi·ªÉu intent & ‚Äúm∆∞·ª£t h√≥a‚Äù c√¢u tr·∫£ l·ªùi) ==============
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# ============== ENV ==============
load_dotenv()

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# Hotline, link ƒëi·ªÅu h∆∞·ªõng, tuy·∫øn tr√™n
HOTLINE_TUYEN_TREN = os.getenv("HOTLINE_TUYEN_TREN", "09xx.xxx.xxx")
LINK_KENH_TELEGRAM = os.getenv("LINK_KENH_TELEGRAM", "https://t.me/your_channel")
LINK_FANPAGE = os.getenv("LINK_FANPAGE", "https://facebook.com/your_fanpage")
LINK_WEBSITE = os.getenv("LINK_WEBSITE", "https://your-website.com")

# ID Telegram c·ªßa tuy·∫øn tr√™n (upline), d·∫°ng s·ªë (string trong .env)
UPLINE_CHAT_ID = os.getenv("UPLINE_CHAT_ID", "")

# L∆∞u c√¢u h·ªèi g·∫ßn nh·∫•t c·ªßa t·ª´ng chat
LAST_USER_TEXT = {}

# Tr·∫°ng th√°i quy tr√¨nh chuy·ªÉn tuy·∫øn tr√™n cho t·ª´ng chat
PENDING_UPLINE_STATE = {}  # "", "waiting_content", "waiting_confirm"
PENDING_UPLINE_TEXT = {}   # n·ªôi dung d·ª± ki·∫øn g·ª≠i tuy·∫øn tr√™n

# Webhook Apps Script ƒë·ªÉ log v√†o Google Sheets
LOG_SHEET_WEBHOOK_URL = os.getenv("LOG_SHEET_WEBHOOK_URL", "")

# ============== KI·ªÇM TRA ENV ==============
if not TELEGRAM_TOKEN:
    raise ValueError("Thi·∫øu TELEGRAM_TOKEN trong .env")

# ============== OpenAI CLIENT ==============
client = None
if OpenAI and OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)

# ============== FLASK APP ==============
app = Flask(__name__)

# ============== ƒê∆Ø·ªúNG D·∫™N JSON ==============
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PRODUCTS_PATH = os.path.join(BASE_DIR, "products.json")
COMBOS_PATH = os.path.join(BASE_DIR, "combos.json")
FAQ_BUY_PATH = os.path.join(BASE_DIR, "faq_buy.json")
FAQ_PAYMENT_PATH = os.path.join(BASE_DIR, "faq_payment.json")
FAQ_BUSINESS_PATH = os.path.join(BASE_DIR, "faq_business.json")

# 2 file m·ªõi:
HEALTH_TAGS_MAP_PATH = os.path.join(BASE_DIR, "health_tags_map.json")
SYNONYMS_PATH = os.path.join(BASE_DIR, "synonyms.json")

# ============== T·∫¢I D·ªÆ LI·ªÜU JSON ==============
def safe_load_json(path, default=None):
    if default is None:
        default = {}
    try:
        if not os.path.exists(path):
            return default
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"[WARN] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c JSON {path}: {e}")
        return default

def extract_list(data, key=None):
    """
    combos.json: { "combos": [ ... ] }
    products.json: { "products": [ ... ] }
    """
    if isinstance(data, list):
        return data
    if isinstance(data, dict) and key and isinstance(data.get(key), list):
        return data[key]
    return []

# ƒë·ªçc d·ªØ li·ªáu th·∫≠t s·ª± d√πng
combos_raw = safe_load_json(COMBOS_PATH, default={"combos": []})
products_raw = safe_load_json(PRODUCTS_PATH, default={"products": []})
faq_buy_data = safe_load_json(FAQ_BUY_PATH, default=[])
faq_payment_data = safe_load_json(FAQ_PAYMENT_PATH, default=[])
faq_business_data = safe_load_json(FAQ_BUSINESS_PATH, default=[])

combos_list = extract_list(combos_raw, "combos")
products_list = extract_list(products_raw, "products")

# 2 file m·ªõi
health_tags_map_data = safe_load_json(HEALTH_TAGS_MAP_PATH, default={})
synonyms_data = safe_load_json(SYNONYMS_PATH, default={})

# ============== H√ÄM TI·ªÜN √çCH CHUNG ==============
def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = text.lower().strip()
    text = unicodedata.normalize("NFD", text)
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")
    return text

def strip_markdown(text: str) -> str:
    """
    Lo·∫°i b·ªè c√°c k√Ω hi·ªáu markdown ƒë∆°n gi·∫£n nh∆∞ **bold**, *italic* trong chu·ªói.
    Kh√¥ng ƒë·ªông v√†o th·∫ª HTML (<b>...</b>) m√† Telegram ƒëang d√πng.
    """
    if not isinstance(text, str):
        return text
    text = re.sub(r"\*\*(.*?)\*\*", r"\1", text)
    text = re.sub(r"\*(.*?)\*", r"\1", text)
    text = text.replace("*", "")
    return text.strip()

def text_contains(text: str, keyword: str) -> bool:
    return normalize_text(keyword) in normalize_text(text)

def send_telegram_message(chat_id, text, reply_to_message_id=None, parse_mode="HTML"):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": text,
        "parse_mode": parse_mode,
        "disable_web_page_preview": False,
    }
    if reply_to_message_id:
        payload["reply_to_message_id"] = reply_to_message_id

    try:
        resp = requests.post(url, json=payload, timeout=15)
        if resp.status_code != 200:
            print("[ERROR] Telegram sendMessage:", resp.text)
    except Exception as e:
        print("[ERROR] G·ª≠i tin nh·∫Øn Telegram l·ªói:", e)

def log_to_sheet(payload: dict):
    if not LOG_SHEET_WEBHOOK_URL:
        return
    try:
        requests.post(LOG_SHEET_WEBHOOK_URL, json=payload, timeout=15)
    except Exception as e:
        print("[WARN] Log sheet l·ªói:", e)

# ============== ƒê·ªíNG B·ªò SYNONYMS & HEALTH TAGS ==============
def apply_synonyms(text: str) -> str:
    """
    Thay th·∫ø c√°c c·ª•m t·ª´ theo synonyms.json (bao t·ª≠ -> d·∫° d√†y, v.v.)
    Kh√¥ng ph√° v·ª° n·ªôi dung, ch·ªâ chu·∫©n h√≥a c√°ch g·ªçi.
    """
    if not text or not isinstance(synonyms_data, dict):
        return text
    result = text
    for k, v in synonyms_data.items():
        if not k or not v:
            continue
        try:
            pattern = re.compile(re.escape(k), flags=re.IGNORECASE)
            result = pattern.sub(v, result)
        except re.error:
            continue
    return result

def expand_health_issue(health_issue: str):
    """
    T·ª´ 1 c√¢u/ c·ª•m 'v·∫•n ƒë·ªÅ s·ª©c kho·∫ª' ‚Üí tr·∫£ v·ªÅ list:
    - [c√¢u g·ªëc, c√¢u sau khi √°p synonyms, c√°c health_tags trong health_tags_map n·∫øu match]
    """
    res = []
    if not health_issue:
        return res

    base = health_issue.strip()
    if base:
        res.append(base)

    syn = apply_synonyms(base)
    if syn and syn not in res:
        res.append(syn)

    try:
        h_norm = normalize_text(base)
        if isinstance(health_tags_map_data, dict):
            for key, tags in health_tags_map_data.items():
                try:
                    key_norm = normalize_text(str(key))
                except Exception:
                    continue
                if not key_norm:
                    continue
                if key_norm in h_norm or h_norm in key_norm:
                    if isinstance(tags, list):
                        for t in tags:
                            if t and t not in res:
                                res.append(t)
                    else:
                        if tags and tags not in res:
                            res.append(tags)
    except Exception as e:
        print("[WARN] expand_health_issue:", e)

    return res

# ============== T√åM KI·∫æM S·∫¢N PH·∫®M & COMBO ==============
def search_combo_by_health_issue(health_issue: str):
    if not health_issue:
        return None

    issues = expand_health_issue(health_issue)
    if not issues:
        issues = [health_issue]

    best_score = 0
    best_combo = None

    for combo in combos_list:
        name = combo.get("name", "")
        aliases = combo.get("aliases", [])
        health_tags = combo.get("health_tags", [])
        fields = [name] + aliases + health_tags

        score = 0
        for issue in issues:
            i_norm = normalize_text(issue)
            for field in fields:
                if text_contains(field, i_norm) or text_contains(i_norm, field):
                    score += 1

        if score > best_score:
            best_score = score
            best_combo = combo

    return best_combo

def search_product_by_health_issue(health_issue: str):
    if not health_issue:
        return []

    issues = expand_health_issue(health_issue)
    if not issues:
        issues = [health_issue]

    results = []
    for p in products_list:
        fields = []
        fields.append(p.get("name", ""))
        fields.extend(p.get("aliases", []))
        fields.extend(p.get("health_tags", []))
        main_tag = p.get("main_health_tag")
        if main_tag:
            fields.append(main_tag)

        match = False
        for issue in issues:
            i_norm = normalize_text(issue)
            for field in fields:
                if text_contains(field, i_norm) or text_contains(i_norm, field):
                    match = True
                    break
            if match:
                break

        if match:
            results.append(p)

    return results[:3]

def search_product_by_name_or_code(query: str):
    if not query:
        return None
    query = apply_synonyms(query)
    q_norm = normalize_text(query)
    best_score = 0
    best_product = None

    for p in products_list:
        code = p.get("code", "")
        name = p.get("name", "")
        aliases = p.get("aliases", [])
        fields = [code, name] + aliases
        score = 0
        for field in fields:
            if not field:
                continue
            if text_contains(field, q_norm) or text_contains(q_norm, field):
                score += 1
        if score > best_score:
            best_score = score
            best_product = p

    return best_product

# ============== OPENAI ‚Äì PH√ÇN T√çCH INTENT & NHU C·∫¶U ==============
def classify_intent_with_openai(user_text: str) -> dict:
    """
    D√πng OpenAI ƒë·ªÉ ph√¢n t√≠ch:
    - intent
    - health_issue
    - product_query
    - needs
    - ask_upline
    Tr·∫£ v·ªÅ dict chu·∫©n.
    """
    base_result = {
        "intent": "SMALL_TALK",
        "health_issue": None,
        "product_query": None,
        "needs": [],
        "ask_upline": False,
        "raw_reasoning": "",
    }

    if not client:
        # N·∫øu kh√¥ng c√≥ OpenAI th√¨ fallback keyword ƒë∆°n gi·∫£n
        t_raw = apply_synonyms(user_text or "")
        t = normalize_text(t_raw)

        # G·∫∑p tuy·∫øn tr√™n
        if any(k in t for k in [
            "ket noi tuyen tren",
            "gap tuyen tren",
            "muon noi voi tuyen tren",
            "chuyen cho tuyen tren",
            "tuyen tren ho tro",
            "nhan tuyen tren"
        ]):
            base_result["intent"] = "BUSINESS_QUESTION"
            base_result["ask_upline"] = True
            return base_result

        if any(k in t for k in ["tieu duong", "dai thao duong"]):
            base_result["intent"] = "HEALTH_COMBO"
            base_result["health_issue"] = "ti·ªÉu ƒë∆∞·ªùng"
        elif any(k in t for k in ["da day", "d·∫° d√†y", "bao tu", "bao t·ª≠", "trao nguoc"]):
            base_result["intent"] = "HEALTH_PRODUCT"
            base_result["health_issue"] = "ƒëau d·∫° d√†y / d·∫° d√†y"
        elif any(k in t for k in ["mua hang", "dat hang", "ƒë·∫∑t h√†ng", "mua nhu the nao", "mua nh∆∞ th·∫ø n√†o"]):
            base_result["intent"] = "HOW_TO_BUY"
        elif any(k in t for k in ["thanh toan", "thanh to√°n", "chuyen khoan", "chuy·ªÉn kho·∫£n"]):
            base_result["intent"] = "HOW_TO_PAY"
        elif any(k in t for k in ["fanpage", "kenh", "k√™nh", "website", "trang web"]):
            base_result["intent"] = "NAVIGATION"
        elif any(k in t for k in ["chinh sach", "hoa hong", "kinh doanh", "th∆∞·ªüng", "chi·∫øt kh·∫•u"]):
            base_result["intent"] = "BUSINESS_QUESTION"
        return base_result

    system_prompt = """
B·∫°n l√† tr·ª£ l√Ω AI n·ªôi b·ªô h·ªó tr·ª£ ƒë·ªôi ng≈© t∆∞ v·∫•n vi√™n (TVV) c·ªßa c√¥ng ty th·ª±c ph·∫©m chƒÉm s√≥c s·ª©c kh·ªèe.
Nhi·ªám v·ª•: ph√¢n t√≠ch c√¢u h·ªèi v√† tr·∫£ v·ªÅ JSON theo c·∫•u tr√∫c.

C√°c INTENT ch√≠nh:
- HEALTH_COMBO: TVV h·ªèi combo cho m·ªôt v·∫•n ƒë·ªÅ s·ª©c kh·ªèe (v√≠ d·ª•: ti·ªÉu ƒë∆∞·ªùng, huy·∫øt √°p, m·ª° m√°u...)
- HEALTH_PRODUCT: TVV h·ªèi s·∫£n ph·∫©m l·∫ª cho m·ªôt v·∫•n ƒë·ªÅ s·ª©c kh·ªèe.
- PRODUCT_DETAIL: TVV h·ªèi th√¥ng tin chi ti·∫øt v·ªÅ m·ªôt s·∫£n ph·∫©m c·ª• th·ªÉ (theo m√£ ho·∫∑c t√™n).
- HOW_TO_BUY: H·ªèi c√°ch mua h√†ng, ƒë·∫∑t h√†ng, quy tr√¨nh.
- HOW_TO_PAY: H·ªèi v·ªÅ c√°ch thanh to√°n, chuy·ªÉn kho·∫£n, COD.
- BUSINESS_QUESTION: H·ªèi v·ªÅ ch√≠nh s√°ch kinh doanh, hoa h·ªìng, chi·∫øt kh·∫•u, th∆∞·ªüng, quy ƒë·ªãnh n·ªôi b·ªô.
- NAVIGATION: H·ªèi xin link fanpage, k√™nh telegram, website, group ch√≠nh th·ª©c.
- SMALL_TALK: Ch√†o h·ªèi, c·∫£m ∆°n, c√¢u chuy·ªán chung chung.
- N·∫øu TVV n√≥i c√°c c√¢u nh∆∞: "k·∫øt n·ªëi tuy·∫øn tr√™n", "anh mu·ªën g·∫∑p tuy·∫øn tr√™n", 
  "nh·ªù tuy·∫øn tr√™n tr·∫£ l·ªùi gi√∫p", "chuy·ªÉn c√¢u n√†y cho tuy·∫øn tr√™n", 
  th√¨:
  + intent = "BUSINESS_QUESTION"
  + ask_upline = true
  + health_issue c√≥ th·ªÉ ƒë·ªÉ null

Tr∆∞·ªùng "needs" l√† danh s√°ch c√°c nhu c·∫ßu c·ª• th·ªÉ trong c√πng 1 c√¢u:
- "combo": c·∫ßn t√™n combo
- "products": c·∫ßn danh s√°ch s·∫£n ph·∫©m trong combo
- "usage": c·∫ßn c√°ch d√πng/c√°ch u·ªëng
- "duration": c·∫ßn th·ªùi gian d√πng bao l√¢u ƒë·ªÉ c√≥ k·∫øt qu·∫£
- "product_links": c·∫ßn link s·∫£n ph·∫©m
- "benefits": c·∫ßn l·ª£i √≠ch/c√¥ng d·ª•ng
- "ingredients": c·∫ßn th√†nh ph·∫ßn s·∫£n ph·∫©m
- "how_to_buy": c·∫ßn h∆∞·ªõng d·∫´n mua h√†ng
- "how_to_pay": c·∫ßn h∆∞·ªõng d·∫´n thanh to√°n

Tr∆∞·ªùng "ask_upline":
- true: n·∫øu c√¢u h·ªèi thu·ªôc d·∫°ng BUSINESS_QUESTION kh√≥ ho·∫∑c nh·∫°y c·∫£m, n√™n chuy·ªÉn tuy·∫øn tr√™n.
- false: c√≤n l·∫°i.

Tr·∫£ v·ªÅ JSON v·ªõi c√°c field:
{
  "intent": "...",
  "health_issue": "... ho·∫∑c null",
  "product_query": "... ho·∫∑c null",
  "needs": [...],
  "ask_upline": false,
  "raw_reasoning": "gi·∫£i th√≠ch ng·∫Øn g·ªçn v√¨ sao ph√¢n lo·∫°i nh∆∞ v·∫≠y"
}

Lu√¥n tr·∫£ v·ªÅ ƒë√∫ng d·∫°ng JSON h·ª£p l·ªá.
"""

    # √Åp synonyms v√†o text tr∆∞·ªõc khi g·ª≠i l√™n OpenAI cho d·ªÖ hi·ªÉu
    processed_text = apply_synonyms(user_text or "")

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": processed_text},
            ],
        )
        content = resp.choices[0].message.content
        data = json.loads(content)
        for k, v in base_result.items():
            if k not in data:
                data[k] = v

        # Chu·∫©n h√≥a health_issue b·∫±ng synonyms lu√¥n
        if data.get("health_issue"):
            data["health_issue"] = apply_synonyms(data["health_issue"])

        return data
    except Exception as e:
        print("[ERROR] OpenAI classify_intent:", e)
        return base_result

# ============== BUILD C√ÇU TR·∫¢ L·ªúI ==============
def format_combo_reply(combo, needs, health_issue):
    if not combo:
        return (
            f"Hi·ªán t·∫°i em ch∆∞a t√¨m th·∫•y combo ph√π h·ª£p cho v·∫•n ƒë·ªÅ: <b>{health_issue}</b>.\n"
            "Anh/ch·ªã m√¥ t·∫£ r√µ h∆°n t√¨nh tr·∫°ng s·ª©c kho·∫ª ƒë·ªÉ em h·ªó tr·ª£ ch√≠nh x√°c h∆°n nh√©."
        )

    raw_name = combo.get("name", "Combo ph√π h·ª£p")
    raw_header_text = combo.get("header_text", "")
    duration_text = combo.get("duration_text", "")
    combo_url = combo.get("combo_url", "")
    products = combo.get("products", [])

    name = strip_markdown(raw_name)
    header_text = strip_markdown(raw_header_text)
    duration_text = strip_markdown(duration_text)

    lines = []
    lines.append(f"üéØ <b>{name}</b>")
    if header_text:
        lines.append(f"üìå {header_text}")

    if products:
        lines.append("\nüß© <b>C√°c s·∫£n ph·∫©m trong combo:</b>")
        for idx, p in enumerate(products, start=1):
            pname_combo = p.get("name") or p.get("product_name") or p.get("product_code") or "S·∫£n ph·∫©m"
            pname_combo = strip_markdown(pname_combo)

            product_detail = None
            for prod in products_list:
                if normalize_text(prod.get("name", "")) == normalize_text(pname_combo) or \
                   normalize_text(prod.get("code", "")) == normalize_text(p.get("code", "")):
                    product_detail = prod
                    break

            price_text = strip_markdown(product_detail.get("price_text", "")) if product_detail else ""
            usage = strip_markdown(product_detail.get("usage_text", "")) if product_detail else ""
            product_url = (product_detail.get("product_url", "") or "").strip() if product_detail else ""
            role_text = strip_markdown(p.get("role_text", "")) if p.get("role_text") else ""
            dose_text = strip_markdown(p.get("dose_text", "")) if p.get("dose_text") else ""

            block_lines = []
            block_lines.append(f"\n<b>{idx}. {pname_combo}</b>")
            if role_text:
                block_lines.append(f"‚ñ™Ô∏è C√¥ng d·ª•ng ch√≠nh: {role_text}")
            if price_text:
                block_lines.append(f"üíµ Gi√° tham kh·∫£o: {price_text}")
            if dose_text:
                block_lines.append(f"üíä C√°ch d√πng (trong combo): {dose_text}")
            elif usage:
                block_lines.append(f"üíä C√°ch d√πng g·ª£i √Ω: {usage}")
            if product_url:
                block_lines.append(f"üîó Link s·∫£n ph·∫©m: {product_url}")
            else:
                block_lines.append(
                    "‚ö† S·∫£n ph·∫©m n√†y hi·ªán <b>kh√¥ng c√≥ link tr√™n h·ªá th·ªëng</b>, "
                    "c√≥ th·ªÉ ƒëang t·∫°m h·∫øt h√†ng ho·∫∑c ch∆∞a m·ªü b√°n online. "
                    "Anh/ch·ªã TVV ki·ªÉm tra l·∫°i kho/trang web tr∆∞·ªõc khi t∆∞ v·∫•n gi√∫p em nh√©."
                )

            lines.append("\n".join(block_lines))

    if duration_text:
        lines.append(f"\n‚è± <b>Th·ªùi gian khuy·∫øn ngh·ªã:</b> {duration_text}")
    if combo_url:
        lines.append(f"\nüõí <b>Link combo:</b> {combo_url}")

    lines.append(
        "\n‚ö†Ô∏è <i>L∆∞u √Ω: ƒê√¢y l√† s·∫£n ph·∫©m h·ªó tr·ª£, kh√¥ng thay th·∫ø thu·ªëc ƒëi·ªÅu tr·ªã. "
        "TVV n√™n h·ªèi k·ªπ t√¨nh tr·∫°ng b·ªánh v√† thu·ªëc kh√°ch ƒëang d√πng tr∆∞·ªõc khi t∆∞ v·∫•n, "
        "ƒë·∫∑c bi·ªát v·ªõi b·ªánh n·ªÅn n·∫∑ng ho·∫∑c ƒëang ƒëi·ªÅu tr·ªã chuy√™n khoa.</i>"
    )
    return "\n".join(lines)

def format_product_reply(product, needs, health_issue=None):
    if not product:
        if health_issue:
            return (
                f"Em ch∆∞a t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p trong d·ªØ li·ªáu cho v·∫•n ƒë·ªÅ: <b>{health_issue}</b>.\n"
                "Anh/ch·ªã th·ª≠ m√¥ t·∫£ r√µ h∆°n tri·ªáu ch·ª©ng ho·∫∑c xin combo t·ªïng th·ªÉ ƒë·ªÉ t∆∞ v·∫•n d·ªÖ h∆°n nh√©."
            )
        return "Em ch∆∞a t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p trong d·ªØ li·ªáu. Anh/ch·ªã ki·ªÉm tra l·∫°i t√™n ho·∫∑c m√£ s·∫£n ph·∫©m gi√∫p em nh√©."

    name = strip_markdown(product.get("name", "S·∫£n ph·∫©m"))
    code = strip_markdown(product.get("code", ""))
    ingredients = strip_markdown(product.get("ingredients_text", ""))
    benefits = strip_markdown(product.get("benefits_text", ""))
    usage = strip_markdown(product.get("usage_text", ""))
    price_text = strip_markdown(product.get("price_text", ""))
    duration_text = strip_markdown(product.get("duration_text", ""))
    product_url = (product.get("product_url", "") or "").strip()
    warnings = strip_markdown(product.get("notes_for_tvv", ""))

    lines = []
    title = f"<b>{name}</b>"
    if code:
        title += f" (M√£: {code})"
    lines.append(title)

    if price_text:
        lines.append(f"üí∞ Gi√° tham kh·∫£o: {price_text}")

    if (not needs) or ("ingredients" in needs):
        if ingredients:
            lines.append("")
            lines.append(f"<b>Th√†nh ph·∫ßn ch√≠nh:</b> {ingredients}")

    if (not needs) or ("benefits" in needs):
        if benefits:
            lines.append("")
            lines.append("<b>L·ª£i √≠ch n·ªïi b·∫≠t:</b>")
            lines.append(benefits)

    if (not needs) or ("usage" in needs):
        if usage:
            lines.append("")
            lines.append(f"<b>C√°ch d√πng khuy·∫øn ngh·ªã:</b> {usage}")

    if (not needs) or ("duration" in needs):
        if duration_text:
            lines.append("")
            lines.append(f"<b>Th·ªùi gian s·ª≠ d·ª•ng n√™n duy tr√¨:</b> {duration_text}")

    if (not needs) or ("product_links" in needs):
        if product_url:
            lines.append("")
            lines.append(f"üîó <b>Link s·∫£n ph·∫©m:</b> {product_url}")

    if warnings:
        lines.append("")
        lines.append(f"‚ö† <b>L∆∞u √Ω cho TVV:</b> {warnings}")

    lines.append("")
    lines.append(
        "Anh/ch·ªã TVV l∆∞u √Ω t∆∞ v·∫•n r√µ ƒë√¢y l√† s·∫£n ph·∫©m h·ªó tr·ª£, kh√¥ng thay th·∫ø thu·ªëc ƒëi·ªÅu tr·ªã, "
        "khuy·∫øn kh√≠ch kh√°ch tham kh·∫£o √Ω ki·∫øn b√°c sƒ© n·∫øu ƒëang d√πng thu·ªëc ho·∫∑c c√≥ b·ªánh n·ªÅn n·∫∑ng."
    )
    return "\n".join(lines)

def format_faq_reply(faq_list, key_field="title"):
    if not faq_list:
        return "Hi·ªán t·∫°i em ch∆∞a c√≥ d·ªØ li·ªáu h∆∞·ªõng d·∫´n chi ti·∫øt trong h·ªá th·ªëng. Anh/ch·ªã gi√∫p em li√™n h·ªá tuy·∫øn tr√™n ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ nh√©."

    if all(isinstance(x, str) for x in faq_list):
        return "\n".join(faq_list)

    lines = []
    for i, item in enumerate(faq_list, start=1):
        if isinstance(item, str):
            lines.append(item)
        elif isinstance(item, dict):
            title = strip_markdown(item.get(key_field, f"B∆∞·ªõc {i}"))
            content = strip_markdown(item.get("content", ""))
            line = f"{i}. <b>{title}</b>"
            if content:
                line += f"\n   {content}"
            lines.append(line)
    return "\n\n".join(lines)

def format_navigation_reply():
    lines = []
    lines.append("<b>C√°c k√™nh ch√≠nh th·ª©c c·ªßa c√¥ng ty:</b>")
    if LINK_KENH_TELEGRAM:
        lines.append(f"üì¢ K√™nh Telegram: {LINK_KENH_TELEGRAM}")
    if LINK_FANPAGE:
        lines.append(f"üëç Fanpage Facebook: {LINK_FANPAGE}")
    if LINK_WEBSITE:
        lines.append(f"üåê Website: {LINK_WEBSITE}")
    lines.append("")
    lines.append("Anh/ch·ªã TVV nh·ªõ ∆∞u ti√™n d·∫´n kh√°ch v√†o c√°c k√™nh ch√≠nh th·ª©c n√†y ƒë·ªÉ theo d√µi ch∆∞∆°ng tr√¨nh v√† th√¥ng tin m·ªõi nh·∫•t nh√©.")
    return "\n".join(lines)

# ============== X·ª¨ L√ù C√ÇU H·ªéI KINH DOANH & CHUY·ªÇN TUY·∫æN TR√äN ==============
def match_business_faq(user_text: str):
    """
    T√¨m c√¢u tr·∫£ l·ªùi trong faq_business_data n·∫øu c√≥.
    C·∫•u tr√∫c g·ª£i √Ω: [{"q_keywords":["hoa h·ªìng","chi·∫øt kh·∫•u"], "answer":"..."}]
    """
    if not faq_business_data:
        return None

    t_raw = apply_synonyms(user_text or "")
    t = normalize_text(t_raw)

    for item in faq_business_data:
        try:
            keywords = item.get("q_keywords", [])
            if not keywords:
                continue
            if all(normalize_text(k) in t for k in keywords):
                return item.get("answer")
        except Exception:
            continue
    return None


def escalate_to_upline(chat_id, username, main_question, extra_note=None):
    """
    G·ª≠i c√¢u h·ªèi l√™n tuy·∫øn tr√™n, log l·∫°i.
    - main_question: c√¢u h·ªèi ch√≠nh (th∆∞·ªùng l√† c√¢u h·ªèi ngay tr∆∞·ªõc khi TVV n√≥i "k·∫øt n·ªëi tuy·∫øn tr√™n")
    - extra_note: c√¢u TVV v·ª´a n√≥i khi y√™u c·∫ßu k·∫øt n·ªëi (tu·ª≥ ch·ªçn)
    """
    if not UPLINE_CHAT_ID:
        return (
            "Hi·ªán t·∫°i em ch∆∞a c·∫•u h√¨nh tuy·∫øn tr√™n trong h·ªá th·ªëng. "
            "Anh/ch·ªã vui l√≤ng li√™n h·ªá tr·ª±c ti·∫øp l√£nh ƒë·∫°o ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
        )

    msg_lines = [
        "üì® <b>Y√äU C·∫¶U H·ªñ TR·ª¢ TUY·∫æN TR√äN</b>",
        "",
        f"üë§ TVV: @{username if username else 'Kh√¥ng r√µ'}",
        f"üí¨ Chat ID: <code>{chat_id}</code>",
        "",
    ]

    if main_question:
        msg_lines.append("‚ùì <b>C√¢u h·ªèi ch√≠nh c·ªßa TVV:</b>")
        msg_lines.append(main_question)
        msg_lines.append("")
    if extra_note and extra_note.strip() != (main_question or "").strip():
        msg_lines.append("üìù <b>Ghi ch√∫ th√™m c·ªßa TVV:</b>")
        msg_lines.append(extra_note)
        msg_lines.append("")

    msg = "\n".join(msg_lines)
    send_telegram_message(UPLINE_CHAT_ID, msg, parse_mode="HTML")

    # Tin nh·∫Øn tr·∫£ l·∫°i cho TVV (echo l·∫°i n·ªôi dung ƒë√£ g·ª≠i)
    if main_question:
        return (
            "Em ƒë√£ g·ª≠i n·ªôi dung sau l√™n tuy·∫øn tr√™n gi√∫p anh/ch·ªã:\n"
            f"\"{main_question}\"\n\n"
            "Khi c√≥ ph·∫£n h·ªìi, em s·∫Ω g·ª≠i l·∫°i ngay ·∫°. üìû"
        )
    else:
        return (
            "Em ƒë√£ chuy·ªÉn y√™u c·∫ßu c·ªßa anh/ch·ªã l√™n tuy·∫øn tr√™n ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£. "
            "Khi c√≥ ph·∫£n h·ªìi, em s·∫Ω b√°o l·∫°i ngay ·∫°. üìû"
        )


def handle_upline_reply(upline_text: str):
    """
    X·ª≠ l√Ω l·ªánh /reply t·ª´ tuy·∫øn tr√™n:
    Format: /reply <chat_id> <n·ªôi dung>
    """
    parts = upline_text.split(maxsplit=2)
    if len(parts) < 3:
        return None, "Sai c√∫ ph√°p. D√πng: /reply <chat_id> <n·ªôi dung>"

    _, chat_id_str, content = parts
    if not chat_id_str.isdigit():
        return None, "Chat ID ph·∫£i l√† s·ªë. V√≠ d·ª•: /reply 123456789 N·ªôi dung tr·∫£ l·ªùi"

    return int(chat_id_str), content

# ============== X·ª¨ L√ù LOGIC CH√çNH ==============
def build_ai_style_reply(user_text: str, core_answer: str) -> str:
    """
    Nh·ªù OpenAI ch·ªânh c√¢u tr·∫£ l·ªùi cho m·ªÅm m·∫°i h∆°n, gi·ªØ nguy√™n th√¥ng tin ch√≠nh.
    N·∫øu kh√¥ng c√≥ OpenAI, tr·∫£ v·ªÅ core_answer lu√¥n.
    L∆ØU √ù: Ch·ªâ d√πng HTML (<b>, <i>...), KH√îNG d√πng Markdown (** **, * *).
    """
    if not client:
        return core_answer

    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω AI n·ªôi b·ªô, x∆∞ng h√¥ "em" v·ªõi TVV, TVV l√† "anh/ch·ªã".

Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán, r√µ r√†ng, d·ªÖ ƒë·ªçc.
- CH·ªà d√πng ƒë·ªãnh d·∫°ng HTML d√†nh cho Telegram: <b>...</b>, <i>...</i>.
- KH√îNG d√πng Markdown, KH√îNG d√πng **...**, *...* ho·∫∑c b·∫•t k·ª≥ k√Ω t·ª± * ƒë·ªÉ in ƒë·∫≠m.
- Kh√¥ng ƒë∆∞·ª£c xo√° hay b·ªãa th√™m th√¥ng tin v·ªÅ s·∫£n ph·∫©m, li·ªÅu d√πng, gi√°, th·ªùi gian s·ª≠ d·ª•ng.
- Gi·ªØ nguy√™n c√°c link (http/https) n·∫øu c√≥.

C√¢u h·ªèi c·ªßa TVV:
\"\"\"{user_text}\"\"\"

D∆∞·ªõi ƒë√¢y l√† n·ªôi dung c·ªët l√µi c·∫ßn truy·ªÅn ƒë·∫°t, b·∫°n ƒë∆∞·ª£c ph√©p ch·ªânh c√¢u ch·ªØ nh∆∞ng kh√¥ng ƒë∆∞·ª£c b·ªãa th√¥ng tin m·ªõi:
\"\"\"{core_answer}\"\"\"
"""
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "B·∫°n l√† tr·ª£ l√Ω b√°n h√†ng n·ªôi b·ªô cho ƒë·ªôi ng≈© TVV. "
                        "Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán, r√µ r√†ng. "
                        "Kh√¥ng d√πng Markdown, ch·ªâ d√πng HTML (<b>, <i>) n·∫øu c·∫ßn nh·∫•n m·∫°nh."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
        )
        content = resp.choices[0].message.content or core_answer
        # Xo√° to√†n b·ªô d·∫•u **, * m√† OpenAI c√≥ th·ªÉ l·ª° ch√®n
        content = strip_markdown(content)
        return content
    except Exception as e:
        print("[ERROR] OpenAI build_ai_style_reply:", e)
        return core_answer


def handle_user_message(chat_id, text, username=None, msg_id=None):
    """
    H√†m trung t√¢m x·ª≠ l√Ω tin nh·∫Øn t·ª´ TVV.
    Gi·ªØ nguy√™n logic c≈©, th√™m:
      - PENDING_UPLINE: x√°c nh·∫≠n tr∆∞·ªõc khi g·ª≠i tuy·∫øn tr√™n
      - LAST_USER_TEXT: l∆∞u c√¢u g·∫ßn nh·∫•t (cho log, m·ªü r·ªông sau n√†y)
    """
     global LAST_USER_TEXT, PENDING_UPLINE_STATE, PENDING_UPLINE_TEXT

    chat_key = str(chat_id)
    state = PENDING_UPLINE_STATE.get(chat_key, "")
    reply_text_core = ""
    ask_upline = False

   # ===== 1. N·∫øu ƒëang ·ªü b∆∞·ªõc CH·ªú N·ªòI DUNG g·ª≠i tuy·∫øn tr√™n =====
    if state == "waiting_content":
        main_question = text.strip()
        PENDING_UPLINE_TEXT[chat_key] = main_question
        PENDING_UPLINE_STATE[chat_key] = "waiting_confirm"

        reply_text_core = (
            "D·∫°, em ƒë√£ ghi nh·∫≠n n·ªôi dung anh/ch·ªã mu·ªën g·ª≠i tuy·∫øn tr√™n l√†:\n"
            f"\"{main_question}\"\n\n"
            "Anh/ch·ªã xem gi√∫p em ƒë√£ ƒë√∫ng √Ω ch∆∞a. N·∫øu <b>ƒë·ªìng √Ω g·ª≠i</b>, anh/ch·ªã ch·ªâ c·∫ßn tr·∫£ l·ªùi: <b>\"ƒë·ªìng √Ω\"</b> "
            "ho·∫∑c <b>\"ok\"</b>. N·∫øu mu·ªën ch·ªânh s·ª≠a, anh/ch·ªã g√µ l·∫°i n·ªôi dung m·ªõi nh√©."
        )

        final_reply = build_ai_style_reply(text, reply_text_core)
        send_telegram_message(chat_id, final_reply, reply_to_message_id=msg_id)

        # kh√¥ng c·∫≠p nh·∫≠t LAST_USER_TEXT ·ªü b∆∞·ªõc confirm
        return

    # ===== 2. N·∫øu ƒëang ·ªü b∆∞·ªõc CH·ªú X√ÅC NH·∫¨N g·ª≠i tuy·∫øn tr√™n =====
    if state == "waiting_confirm":
        t_norm = normalize_text(text)
        if any(k in t_norm for k in ["dong y", "ƒë·ªìng √Ω", "ok", "oke", "chu·∫©n", "chuan roi"]):
            main_question = PENDING_UPLINE_TEXT.get(chat_key, "").strip()
            reply_text_core = escalate_to_upline(chat_id, username, main_question)
            ask_upline = True

            # reset state
            PENDING_UPLINE_STATE.pop(chat_key, None)
            PENDING_UPLINE_TEXT.pop(chat_key, None)
        else:
            # coi ƒë√¢y l√† n·ªôi dung m·ªõi, c·∫≠p nh·∫≠t l·∫°i r·ªìi y√™u c·∫ßu x√°c nh·∫≠n ti·∫øp
            main_question = text.strip()
            PENDING_UPLINE_TEXT[chat_key] = main_question
            PENDING_UPLINE_STATE[chat_key] = "waiting_confirm"
            reply_text_core = (
                "Em ƒë√£ c·∫≠p nh·∫≠t n·ªôi dung c·∫ßn g·ª≠i tuy·∫øn tr√™n l√†:\n"
                f"\"{main_question}\"\n\n"
                "N·∫øu anh/ch·ªã <b>ƒë·ªìng √Ω</b>, h√£y tr·∫£ l·ªùi: <b>\"ƒë·ªìng √Ω\"</b> ho·∫∑c <b>\"ok\"</b> ƒë·ªÉ em g·ª≠i ƒëi nh√©."
            )

        final_reply = build_ai_style_reply(text, reply_text_core)
        send_telegram_message(chat_id, final_reply, reply_to_message_id=msg_id)
        return

    # ===== 3. Kh√¥ng ·ªü flow tuy·∫øn tr√™n: x·ª≠ l√Ω b√¨nh th∆∞·ªùng =====
    previous_text = LAST_USER_TEXT.get(chat_key)

    intent_info = classify_intent_with_openai(text)
    intent = intent_info.get("intent", "SMALL_TALK")
    health_issue = intent_info.get("health_issue")
    product_query = intent_info.get("product_query")
    needs = intent_info.get("needs") or []
    ask_upline = bool(intent_info.get("ask_upline", False))

    log_payload = {
        "source": "telegram",
        "chat_id": str(chat_id),
        "username": username or "",
        "user_text": text,
        "intent": intent,
        "health_issue": health_issue or "",
        "product_query": product_query or "",
    }

    if intent == "HEALTH_COMBO":
        combo = search_combo_by_health_issue(health_issue or text)
        reply_text_core = format_combo_reply(combo, needs, health_issue or text)

    elif intent == "HEALTH_PRODUCT":
        if product_query:
            product = search_product_by_name_or_code(product_query)
            reply_text_core = format_product_reply(product, needs, health_issue=None)
        else:
            products = search_product_by_health_issue(health_issue or text)
            if not products:
                reply_text_core = format_product_reply(None, needs, health_issue or text)
            elif len(products) == 1:
                reply_text_core = format_product_reply(products[0], needs, health_issue or text)
            else:
                lines = [f"<b>M·ªôt s·ªë s·∫£n ph·∫©m ph√π h·ª£p v·ªõi v·∫•n ƒë·ªÅ {health_issue or text}:</b>"]
                for p in products:
                    name = strip_markdown(p.get("name", "S·∫£n ph·∫©m"))
                    code = strip_markdown(p.get("code", ""))
                    url = (p.get("product_url", "") or "").strip()
                    line = f"‚Ä¢ {name}"
                    if code:
                        line += f" (M√£: {code})"
                    if url:
                        line += f"\n   üîó {url}"
                    lines.append(line)
                lines.append("")
                lines.append("N·∫øu anh/ch·ªã mu·ªën xem chi ti·∫øt s·∫£n ph·∫©m n√†o, h√£y h·ªèi theo t√™n ho·∫∑c m√£ s·∫£n ph·∫©m c·ª• th·ªÉ nh√©.")
                reply_text_core = "\n".join(lines)

    elif intent == "PRODUCT_DETAIL":
        product = search_product_by_name_or_code(product_query or text)
        reply_text_core = format_product_reply(product, needs, health_issue=None)

    elif intent == "HOW_TO_BUY":
        reply_text_core = format_faq_reply(faq_buy_data)

    elif intent == "HOW_TO_PAY":
        reply_text_core = format_faq_reply(faq_payment_data)

    elif intent == "NAVIGATION":
        reply_text_core = format_navigation_reply()

    elif intent == "BUSINESS_QUESTION":
        # th·ª≠ FAQ tr∆∞·ªõc
        faq_answer = match_business_faq(text)
        if faq_answer:
            reply_text_core = faq_answer
        elif ask_upline:
            # b·∫Øt ƒë·∫ßu flow tuy·∫øn tr√™n: CH∆ØA g·ª≠i g√¨ c·∫£
            PENDING_UPLINE_STATE[chat_key] = "waiting_content"
            PENDING_UPLINE_TEXT.pop(chat_key, None)
            reply_text_core = (
                "D·∫°, em s·∫Ω k·∫øt n·ªëi tuy·∫øn tr√™n ƒë·ªÉ h·ªó tr·ª£ anh/ch·ªã.\n\n"
                "Anh/ch·ªã cho em bi·∫øt <b>c·ª• th·ªÉ n·ªôi dung</b> mu·ªën h·ªèi tuy·∫øn tr√™n (t√¨nh hu·ªëng, s·∫£n ph·∫©m/combo, ch√≠nh s√°ch...) "
                "ƒë·ªÉ em g·ª≠i ƒë√∫ng √Ω anh/ch·ªã nh·∫•t nh√©."
            )
        else:
            reply_text_core = (
                "V·∫•n ƒë·ªÅ n√†y thu·ªôc nh√≥m ch√≠nh s√°ch/kinh doanh ho·∫∑c t√¨nh hu·ªëng kh√≥. "
                "N·∫øu anh/ch·ªã mu·ªën, em c√≥ th·ªÉ k·∫øt n·ªëi tuy·∫øn tr√™n ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ tr·ª±c ti·∫øp ·∫°."
            )

    else:
        reply_text_core = (
            "Em l√† tr·ª£ l√Ω AI n·ªôi b·ªô h·ªó tr·ª£ anh/ch·ªã TVV trong vi·ªác t∆∞ v·∫•n s·∫£n ph·∫©m, combo v√† c√°ch chƒÉm s√≥c s·ª©c kho·∫ª.\n\n"
            "Anh/ch·ªã c√≥ th·ªÉ h·ªèi em v·ªÅ:\n"
            "‚Ä¢ Combo cho m·ªôt v·∫•n ƒë·ªÅ s·ª©c kh·ªèe (v√≠ d·ª•: ti·ªÉu ƒë∆∞·ªùng, d·∫° d√†y, x∆∞∆°ng kh·ªõp...)\n"
            "‚Ä¢ Th√¥ng tin chi ti·∫øt m·ªôt s·∫£n ph·∫©m (th√†nh ph·∫ßn, l·ª£i √≠ch, c√°ch d√πng...)\n"
            "‚Ä¢ C√°ch mua h√†ng, thanh to√°n, k√™nh ch√≠nh th·ª©c c·ªßa c√¥ng ty\n"
            "‚Ä¢ Nh·ªØng th·∫Øc m·∫Øc v·ªÅ kinh doanh, ch√≠nh s√°ch (em s·∫Ω h·ªó tr·ª£ chuy·ªÉn tuy·∫øn tr√™n n·∫øu c·∫ßn) üòä"
        )

    log_payload["ask_upline"] = "yes" if ask_upline else "no"
    log_payload["final_answer_preview"] = reply_text_core[:500]
    log_to_sheet(log_payload)

    final_reply = build_ai_style_reply(text, reply_text_core)
    send_telegram_message(chat_id, final_reply, reply_to_message_id=msg_id)

    # c·∫≠p nh·∫≠t c√¢u h·ªèi g·∫ßn nh·∫•t (d√πng cho ph√¢n t√≠ch sau n√†y)
    LAST_USER_TEXT[chat_key] = text

   # ============== ROUTES FLASK ==============
@app.route("/", methods=["GET"])
def index():
    return jsonify({"status": "ok", "message": "Welllab AI Assistant is running."})

@app.route("/webhook", methods=["POST"])
def telegram_webhook():
    update = request.get_json(force=True, silent=True) or {}
    message = update.get("message") or update.get("edited_message")
    if not message:
        return jsonify({"ok": True})

    chat = message.get("chat", {})
    chat_id = chat.get("id")
    from_user = message.get("from", {})
    username = from_user.get("username") or from_user.get("first_name")
    text = message.get("text", "") or ""

    # Tin nh·∫Øn t·ª´ tuy·∫øn tr√™n
    if UPLINE_CHAT_ID and str(chat_id) == str(UPLINE_CHAT_ID):
        if text.startswith("/reply"):
            target_chat_id, content = handle_upline_reply(text)
            if not target_chat_id:
                send_telegram_message(chat_id, content)
            else:
                send_telegram_message(target_chat_id, f"üì£ Ph·∫£n h·ªìi t·ª´ tuy·∫øn tr√™n:\n\n{content}")
                send_telegram_message(chat_id, "ƒê√£ g·ª≠i tr·∫£ l·ªùi cho TVV.")
        else:
            send_telegram_message(
                chat_id,
                "ƒê√¢y l√† k√™nh tuy·∫øn tr√™n. ƒê·ªÉ tr·∫£ l·ªùi TVV, d√πng l·ªánh:\n/reply <chat_id> <n·ªôi dung>",
            )
        return jsonify({"ok": True})

    # /start
    if text.startswith("/start"):
        welcome = (
            "Ch√†o anh/ch·ªã, em l√† <b>Tr·ª£ l√Ω AI Welllab</b> h·ªó tr·ª£ ƒë·ªôi ng≈© TVV üíö\n\n"
            "Anh/ch·ªã c√≥ th·ªÉ h·ªèi em v·ªÅ:\n"
            "‚Ä¢ Combo cho c√°c v·∫•n ƒë·ªÅ s·ª©c kh·ªèe (ti·ªÉu ƒë∆∞·ªùng, d·∫° d√†y, m·ª° m√°u, x∆∞∆°ng kh·ªõp...)\n"
            "‚Ä¢ Th√¥ng tin chi ti·∫øt s·∫£n ph·∫©m (th√†nh ph·∫ßn, l·ª£i √≠ch, c√°ch d√πng...)\n"
            "‚Ä¢ C√°ch mua h√†ng, thanh to√°n, k√™nh ch√≠nh th·ª©c c·ªßa c√¥ng ty\n"
            "‚Ä¢ C√¢u h·ªèi kinh doanh, ch√≠nh s√°ch (em s·∫Ω h·ªó tr·ª£ chuy·ªÉn tuy·∫øn tr√™n n·∫øu c·∫ßn)\n\n"
            "Anh/ch·ªã c·ª© nh·∫Øn t·ª± nhi√™n nh∆∞ ƒëang h·ªèi m·ªôt leader nh√© ü•∞"
        )
        send_telegram_message(chat_id, welcome, reply_to_message_id=message.get("message_id"))
        return jsonify({"ok": True})

    handle_user_message(chat_id, text, username=username, msg_id=message.get("message_id"))
    return jsonify({"ok": True})

# ============== MAIN ==============
if __name__ == "__main__":
    port = int(os.getenv("PORT", "8000"))
    app.run(host="0.0.0.0", port=port)

